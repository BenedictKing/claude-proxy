// Package common æä¾› handlers æ¨¡å—çš„å…¬å…±åŠŸèƒ½
package common

import (
	"bytes"
	"encoding/json"
	"fmt"
	"log"
	"net/http"
	"strings"
	"time"

	"github.com/BenedictKing/claude-proxy/internal/config"
	"github.com/BenedictKing/claude-proxy/internal/providers"
	"github.com/BenedictKing/claude-proxy/internal/scheduler"
	"github.com/BenedictKing/claude-proxy/internal/types"
	"github.com/BenedictKing/claude-proxy/internal/utils"
	"github.com/gin-gonic/gin"
)

// StreamContext æµå¤„ç†ä¸Šä¸‹æ–‡
type StreamContext struct {
	LogBuffer        bytes.Buffer
	OutputTextBuffer bytes.Buffer
	Synthesizer      *utils.StreamSynthesizer
	LoggingEnabled   bool
	ClientGone       bool
	HasUsage         bool
	NeedTokenPatch   bool
	// ç´¯ç§¯çš„ token ç»Ÿè®¡
	CollectedUsage CollectedUsageData
}

// CollectedUsageData ä»æµäº‹ä»¶ä¸­æ”¶é›†çš„ usage æ•°æ®
type CollectedUsageData struct {
	InputTokens              int
	OutputTokens             int
	CacheCreationInputTokens int
	CacheReadInputTokens     int
	// ç¼“å­˜ TTL ç»†åˆ†
	CacheCreation5mInputTokens int
	CacheCreation1hInputTokens int
	CacheTTL                   string // "5m" | "1h" | "mixed"
}

// NewStreamContext åˆ›å»ºæµå¤„ç†ä¸Šä¸‹æ–‡
func NewStreamContext(envCfg *config.EnvConfig) *StreamContext {
	ctx := &StreamContext{
		LoggingEnabled: envCfg.IsDevelopment() && envCfg.EnableResponseLogs,
	}
	if ctx.LoggingEnabled {
		ctx.Synthesizer = utils.NewStreamSynthesizer("claude")
	}
	return ctx
}

// SetupStreamHeaders è®¾ç½®æµå¼å“åº”å¤´
func SetupStreamHeaders(c *gin.Context, resp *http.Response) {
	utils.ForwardResponseHeaders(resp.Header, c.Writer)
	c.Header("Content-Type", "text/event-stream")
	c.Header("Cache-Control", "no-cache")
	c.Header("Connection", "keep-alive")
	c.Header("X-Accel-Buffering", "no")
	c.Status(200)
}

// ProcessStreamEvents å¤„ç†æµäº‹ä»¶å¾ªç¯
// è¿”å›å€¼: error è¡¨ç¤ºæµå¤„ç†è¿‡ç¨‹ä¸­æ˜¯å¦å‘ç”Ÿé”™è¯¯ï¼ˆç”¨äºè°ƒç”¨æ–¹å†³å®šæ˜¯å¦è®°å½•å¤±è´¥æŒ‡æ ‡ï¼‰
func ProcessStreamEvents(
	c *gin.Context,
	w gin.ResponseWriter,
	flusher http.Flusher,
	eventChan <-chan string,
	errChan <-chan error,
	ctx *StreamContext,
	envCfg *config.EnvConfig,
	startTime time.Time,
	requestBody []byte,
	channelScheduler *scheduler.ChannelScheduler,
	upstream *config.UpstreamConfig,
	apiKey string,
) error {
	for {
		select {
		case event, ok := <-eventChan:
			if !ok {
				logStreamCompletion(ctx, envCfg, startTime, channelScheduler, upstream, apiKey)
				return nil
			}
			ProcessStreamEvent(c, w, flusher, event, ctx, envCfg, requestBody)

		case err, ok := <-errChan:
			if !ok {
				continue
			}
			if err != nil {
				log.Printf("ğŸ’¥ æµå¼ä¼ è¾“é”™è¯¯: %v", err)
				logPartialResponse(ctx, envCfg)

				// è®°å½•å¤±è´¥æŒ‡æ ‡
				channelScheduler.RecordFailure(upstream.BaseURL, apiKey, false)

				// å‘å®¢æˆ·ç«¯å‘é€é”™è¯¯äº‹ä»¶ï¼ˆå¦‚æœè¿æ¥ä»ç„¶æœ‰æ•ˆï¼‰
				if !ctx.ClientGone {
					errorEvent := BuildStreamErrorEvent(err)
					w.Write([]byte(errorEvent))
					flusher.Flush()
				}

				return err
			}
		}
	}
}

// ProcessStreamEvent å¤„ç†å•ä¸ªæµäº‹ä»¶
func ProcessStreamEvent(
	c *gin.Context,
	w gin.ResponseWriter,
	flusher http.Flusher,
	event string,
	ctx *StreamContext,
	envCfg *config.EnvConfig,
	requestBody []byte,
) {
	// æå–æ–‡æœ¬ç”¨äºä¼°ç®— token
	ExtractTextFromEvent(event, &ctx.OutputTextBuffer)

	// æ£€æµ‹å¹¶æ”¶é›† usage
	hasUsage, needPatch, usageData := CheckEventUsageStatus(event, envCfg.EnableResponseLogs && envCfg.ShouldLog("debug"))
	if hasUsage {
		if !ctx.HasUsage {
			ctx.HasUsage = true
			ctx.NeedTokenPatch = needPatch
			if envCfg.EnableResponseLogs && envCfg.ShouldLog("debug") && needPatch && !IsMessageDeltaEvent(event) {
				log.Printf("ğŸ”¢ [Stream-Token] æ£€æµ‹åˆ°è™šå‡å€¼, å»¶è¿Ÿåˆ°æµç»“æŸä¿®è¡¥")
			}
		}
		// ç´¯ç§¯æ”¶é›† usage æ•°æ®
		updateCollectedUsage(&ctx.CollectedUsage, usageData)
	}

	// æ—¥å¿—ç¼“å­˜
	if ctx.LoggingEnabled {
		ctx.LogBuffer.WriteString(event)
		if ctx.Synthesizer != nil {
			for _, line := range strings.Split(event, "\n") {
				ctx.Synthesizer.ProcessLine(line)
			}
		}
	}

	// åœ¨ message_stop å‰æ³¨å…¥ usageï¼ˆä¸Šæ¸¸å®Œå…¨æ²¡æœ‰ usage çš„æƒ…å†µï¼‰
	if !ctx.HasUsage && !ctx.ClientGone && IsMessageStopEvent(event) {
		usageEvent := BuildUsageEvent(requestBody, ctx.OutputTextBuffer.String())
		if envCfg.EnableResponseLogs && envCfg.ShouldLog("debug") {
			log.Printf("ğŸ”¢ [Stream-Tokenæ³¨å…¥] ä¸Šæ¸¸æ— usage, æ³¨å…¥æœ¬åœ°ä¼°ç®—äº‹ä»¶")
		}
		w.Write([]byte(usageEvent))
		flusher.Flush()
		ctx.HasUsage = true
	}

	// ä¿®è¡¥ token
	eventToSend := event
	if ctx.NeedTokenPatch && HasEventWithUsage(event) {
		if IsMessageDeltaEvent(event) || IsMessageStopEvent(event) {
			inputTokens := ctx.CollectedUsage.InputTokens
			if inputTokens == 0 {
				inputTokens = utils.EstimateRequestTokens(requestBody)
			}
			outputTokens := ctx.CollectedUsage.OutputTokens
			if outputTokens == 0 {
				outputTokens = utils.EstimateTokens(ctx.OutputTextBuffer.String())
			}
			hasCacheTokens := ctx.CollectedUsage.CacheCreationInputTokens > 0 || ctx.CollectedUsage.CacheReadInputTokens > 0
			eventToSend = PatchTokensInEvent(event, inputTokens, outputTokens, hasCacheTokens, envCfg.EnableResponseLogs && envCfg.ShouldLog("debug"))
			ctx.NeedTokenPatch = false
		}
	}

	// è½¬å‘ç»™å®¢æˆ·ç«¯
	if !ctx.ClientGone {
		if _, err := w.Write([]byte(eventToSend)); err != nil {
			ctx.ClientGone = true
			if !IsClientDisconnectError(err) {
				log.Printf("âš ï¸ æµå¼ä¼ è¾“å†™å…¥é”™è¯¯: %v", err)
			} else if envCfg.ShouldLog("info") {
				log.Printf("â„¹ï¸ å®¢æˆ·ç«¯ä¸­æ–­è¿æ¥ (æ­£å¸¸è¡Œä¸º)ï¼Œç»§ç»­æ¥æ”¶ä¸Šæ¸¸æ•°æ®...")
			}
		} else {
			flusher.Flush()
		}
	}
}

// updateCollectedUsage æ›´æ–°æ”¶é›†çš„ usage æ•°æ®
func updateCollectedUsage(collected *CollectedUsageData, usageData CollectedUsageData) {
	if usageData.InputTokens > collected.InputTokens {
		collected.InputTokens = usageData.InputTokens
	}
	if usageData.OutputTokens > collected.OutputTokens {
		collected.OutputTokens = usageData.OutputTokens
	}
	if usageData.CacheCreationInputTokens > 0 {
		collected.CacheCreationInputTokens = usageData.CacheCreationInputTokens
	}
	if usageData.CacheReadInputTokens > 0 {
		collected.CacheReadInputTokens = usageData.CacheReadInputTokens
	}
	if usageData.CacheCreation5mInputTokens > 0 {
		collected.CacheCreation5mInputTokens = usageData.CacheCreation5mInputTokens
	}
	if usageData.CacheCreation1hInputTokens > 0 {
		collected.CacheCreation1hInputTokens = usageData.CacheCreation1hInputTokens
	}
	if usageData.CacheTTL != "" {
		collected.CacheTTL = usageData.CacheTTL
	}
}

// logStreamCompletion è®°å½•æµå®Œæˆæ—¥å¿—
func logStreamCompletion(ctx *StreamContext, envCfg *config.EnvConfig, startTime time.Time, channelScheduler *scheduler.ChannelScheduler, upstream *config.UpstreamConfig, apiKey string) {
	if envCfg.EnableResponseLogs {
		log.Printf("â±ï¸ æµå¼å“åº”å®Œæˆ: %dms", time.Since(startTime).Milliseconds())
	}

	if envCfg.IsDevelopment() {
		logSynthesizedContent(ctx)
	}

	// å°†ç´¯ç§¯çš„ usage æ•°æ®è½¬æ¢ä¸º *types.Usage
	var usage *types.Usage
	hasUsageData := ctx.CollectedUsage.InputTokens > 0 ||
		ctx.CollectedUsage.OutputTokens > 0 ||
		ctx.CollectedUsage.CacheCreationInputTokens > 0 ||
		ctx.CollectedUsage.CacheReadInputTokens > 0 ||
		ctx.CollectedUsage.CacheCreation5mInputTokens > 0 ||
		ctx.CollectedUsage.CacheCreation1hInputTokens > 0
	if hasUsageData {
		usage = &types.Usage{
			InputTokens:                ctx.CollectedUsage.InputTokens,
			OutputTokens:               ctx.CollectedUsage.OutputTokens,
			CacheCreationInputTokens:   ctx.CollectedUsage.CacheCreationInputTokens,
			CacheReadInputTokens:       ctx.CollectedUsage.CacheReadInputTokens,
			CacheCreation5mInputTokens: ctx.CollectedUsage.CacheCreation5mInputTokens,
			CacheCreation1hInputTokens: ctx.CollectedUsage.CacheCreation1hInputTokens,
			CacheTTL:                   ctx.CollectedUsage.CacheTTL,
		}
	}

	// è®°å½•æˆåŠŸæŒ‡æ ‡
	channelScheduler.RecordSuccessWithUsage(upstream.BaseURL, apiKey, usage, false)
}

// logPartialResponse è®°å½•éƒ¨åˆ†å“åº”æ—¥å¿—
func logPartialResponse(ctx *StreamContext, envCfg *config.EnvConfig) {
	if envCfg.EnableResponseLogs && envCfg.IsDevelopment() {
		logSynthesizedContent(ctx)
	}
}

// logSynthesizedContent è®°å½•åˆæˆå†…å®¹
func logSynthesizedContent(ctx *StreamContext) {
	if ctx.Synthesizer != nil {
		content := ctx.Synthesizer.GetSynthesizedContent()
		if content != "" && !ctx.Synthesizer.IsParseFailed() {
			log.Printf("ğŸ›°ï¸  ä¸Šæ¸¸æµå¼å“åº”åˆæˆå†…å®¹:\n%s", strings.TrimSpace(content))
			return
		}
	}
	if ctx.LogBuffer.Len() > 0 {
		log.Printf("ğŸ›°ï¸  ä¸Šæ¸¸æµå¼å“åº”åŸå§‹å†…å®¹:\n%s", ctx.LogBuffer.String())
	}
}

// IsClientDisconnectError åˆ¤æ–­æ˜¯å¦ä¸ºå®¢æˆ·ç«¯æ–­å¼€è¿æ¥é”™è¯¯
func IsClientDisconnectError(err error) bool {
	msg := err.Error()
	return strings.Contains(msg, "broken pipe") || strings.Contains(msg, "connection reset")
}

// HandleStreamResponse å¤„ç†æµå¼å“åº”ï¼ˆMessages APIï¼‰
func HandleStreamResponse(
	c *gin.Context,
	resp *http.Response,
	provider providers.Provider,
	envCfg *config.EnvConfig,
	startTime time.Time,
	upstream *config.UpstreamConfig,
	requestBody []byte,
	channelScheduler *scheduler.ChannelScheduler,
	apiKey string,
) {
	defer resp.Body.Close()

	eventChan, errChan, err := provider.HandleStreamResponse(resp.Body)
	if err != nil {
		c.JSON(500, gin.H{"error": "Failed to handle stream response"})
		return
	}

	SetupStreamHeaders(c, resp)

	w := c.Writer
	flusher, ok := w.(http.Flusher)
	if !ok {
		log.Printf("âš ï¸ ResponseWriterä¸æ”¯æŒFlushæ¥å£")
		return
	}
	flusher.Flush()

	ctx := NewStreamContext(envCfg)
	ProcessStreamEvents(c, w, flusher, eventChan, errChan, ctx, envCfg, startTime, requestBody, channelScheduler, upstream, apiKey)
}

// ========== Token æ£€æµ‹å’Œä¿®è¡¥ç›¸å…³å‡½æ•° ==========

// CheckEventUsageStatus æ£€æµ‹äº‹ä»¶æ˜¯å¦åŒ…å« usage å­—æ®µ
func CheckEventUsageStatus(event string, enableLog bool) (bool, bool, CollectedUsageData) {
	for _, line := range strings.Split(event, "\n") {
		if !strings.HasPrefix(line, "data: ") {
			continue
		}
		jsonStr := strings.TrimPrefix(line, "data: ")

		var data map[string]interface{}
		if err := json.Unmarshal([]byte(jsonStr), &data); err != nil {
			continue
		}

		// æ£€æŸ¥é¡¶å±‚ usage å­—æ®µ
		if hasUsage, needInputPatch, needOutputPatch := checkUsageFieldsWithPatch(data["usage"]); hasUsage {
			needPatch := needInputPatch || needOutputPatch
			var usageData CollectedUsageData
			if usage, ok := data["usage"].(map[string]interface{}); ok {
				if enableLog {
					logUsageDetection("é¡¶å±‚usage", usage, needPatch)
				}
				usageData = extractUsageFromMap(usage)
			}
			return true, needPatch, usageData
		}

		// æ£€æŸ¥ message.usage
		if msg, ok := data["message"].(map[string]interface{}); ok {
			if hasUsage, needInputPatch, needOutputPatch := checkUsageFieldsWithPatch(msg["usage"]); hasUsage {
				needPatch := needInputPatch || needOutputPatch
				var usageData CollectedUsageData
				if usage, ok := msg["usage"].(map[string]interface{}); ok {
					if enableLog {
						logUsageDetection("message.usage", usage, needPatch)
					}
					usageData = extractUsageFromMap(usage)
				}
				return true, needPatch, usageData
			}
		}
	}
	return false, false, CollectedUsageData{}
}

// checkUsageFieldsWithPatch æ£€æŸ¥ usage å¯¹è±¡æ˜¯å¦åŒ…å« token å­—æ®µ
func checkUsageFieldsWithPatch(usage interface{}) (bool, bool, bool) {
	if u, ok := usage.(map[string]interface{}); ok {
		inputTokens, hasInput := u["input_tokens"]
		outputTokens, hasOutput := u["output_tokens"]
		if hasInput || hasOutput {
			needInputPatch := false
			needOutputPatch := false

			cacheCreation, _ := u["cache_creation_input_tokens"].(float64)
			cacheRead, _ := u["cache_read_input_tokens"].(float64)
			hasCacheTokens := cacheCreation > 0 || cacheRead > 0

			if hasInput {
				if v, ok := inputTokens.(float64); ok && v <= 1 && !hasCacheTokens {
					needInputPatch = true
				}
			}
			if hasOutput {
				if v, ok := outputTokens.(float64); ok && v <= 1 {
					needOutputPatch = true
				}
			}
			return true, needInputPatch, needOutputPatch
		}
	}
	return false, false, false
}

// extractUsageFromMap ä» usage map ä¸­æå– token æ•°æ®
func extractUsageFromMap(usage map[string]interface{}) CollectedUsageData {
	var data CollectedUsageData

	if v, ok := usage["input_tokens"].(float64); ok {
		data.InputTokens = int(v)
	}
	if v, ok := usage["output_tokens"].(float64); ok {
		data.OutputTokens = int(v)
	}
	if v, ok := usage["cache_creation_input_tokens"].(float64); ok {
		data.CacheCreationInputTokens = int(v)
	}
	if v, ok := usage["cache_read_input_tokens"].(float64); ok {
		data.CacheReadInputTokens = int(v)
	}

	var has5m, has1h bool
	if v, ok := usage["cache_creation_5m_input_tokens"].(float64); ok {
		data.CacheCreation5mInputTokens = int(v)
		has5m = data.CacheCreation5mInputTokens > 0
	}
	if v, ok := usage["cache_creation_1h_input_tokens"].(float64); ok {
		data.CacheCreation1hInputTokens = int(v)
		has1h = data.CacheCreation1hInputTokens > 0
	}

	if has5m && has1h {
		data.CacheTTL = "mixed"
	} else if has1h {
		data.CacheTTL = "1h"
	} else if has5m {
		data.CacheTTL = "5m"
	}

	return data
}

// logUsageDetection ç»Ÿä¸€æ ¼å¼è¾“å‡º usage æ£€æµ‹æ—¥å¿—
func logUsageDetection(location string, usage map[string]interface{}, needPatch bool) {
	inputTokens := usage["input_tokens"]
	outputTokens := usage["output_tokens"]
	cacheCreation, _ := usage["cache_creation_input_tokens"].(float64)
	cacheRead, _ := usage["cache_read_input_tokens"].(float64)

	log.Printf("ğŸ”¢ [Stream-Tokenæ£€æµ‹] %s: InputTokens=%v, OutputTokens=%v, CacheCreation=%.0f, CacheRead=%.0f, éœ€è¡¥å…¨=%v",
		location, inputTokens, outputTokens, cacheCreation, cacheRead, needPatch)
}

// HasEventWithUsage æ£€æŸ¥äº‹ä»¶æ˜¯å¦åŒ…å« usage å­—æ®µ
func HasEventWithUsage(event string) bool {
	for _, line := range strings.Split(event, "\n") {
		if !strings.HasPrefix(line, "data: ") {
			continue
		}
		jsonStr := strings.TrimPrefix(line, "data: ")

		var data map[string]interface{}
		if err := json.Unmarshal([]byte(jsonStr), &data); err != nil {
			continue
		}

		if _, ok := data["usage"].(map[string]interface{}); ok {
			return true
		}

		if msg, ok := data["message"].(map[string]interface{}); ok {
			if _, ok := msg["usage"].(map[string]interface{}); ok {
				return true
			}
		}
	}
	return false
}

// PatchTokensInEvent ä¿®è¡¥äº‹ä»¶ä¸­çš„ token å­—æ®µ
func PatchTokensInEvent(event string, estimatedInputTokens, estimatedOutputTokens int, hasCacheTokens bool, enableLog bool) string {
	var result strings.Builder
	lines := strings.Split(event, "\n")

	for _, line := range lines {
		if !strings.HasPrefix(line, "data: ") {
			result.WriteString(line)
			result.WriteString("\n")
			continue
		}

		jsonStr := strings.TrimPrefix(line, "data: ")
		var data map[string]interface{}
		if err := json.Unmarshal([]byte(jsonStr), &data); err != nil {
			result.WriteString(line)
			result.WriteString("\n")
			continue
		}

		// ä¿®è¡¥é¡¶å±‚ usage
		if usage, ok := data["usage"].(map[string]interface{}); ok {
			patchUsageFieldsWithLog(usage, estimatedInputTokens, estimatedOutputTokens, hasCacheTokens, enableLog, "é¡¶å±‚usage")
		}

		// ä¿®è¡¥ message.usage
		if msg, ok := data["message"].(map[string]interface{}); ok {
			if usage, ok := msg["usage"].(map[string]interface{}); ok {
				patchUsageFieldsWithLog(usage, estimatedInputTokens, estimatedOutputTokens, hasCacheTokens, enableLog, "message.usage")
			}
		}

		patchedJSON, err := json.Marshal(data)
		if err != nil {
			result.WriteString(line)
			result.WriteString("\n")
			continue
		}

		result.WriteString("data: ")
		result.Write(patchedJSON)
		result.WriteString("\n")
	}

	return result.String()
}

// patchUsageFieldsWithLog ä¿®è¡¥ usage å¯¹è±¡ä¸­çš„ token å­—æ®µ
func patchUsageFieldsWithLog(usage map[string]interface{}, estimatedInput, estimatedOutput int, hasCacheTokens bool, enableLog bool, location string) {
	originalInput := usage["input_tokens"]
	originalOutput := usage["output_tokens"]
	inputPatched := false
	outputPatched := false

	cacheCreation, _ := usage["cache_creation_input_tokens"].(float64)
	cacheRead, _ := usage["cache_read_input_tokens"].(float64)
	cacheCreation5m, _ := usage["cache_creation_5m_input_tokens"].(float64)
	cacheCreation1h, _ := usage["cache_creation_1h_input_tokens"].(float64)
	cacheTTL, _ := usage["cache_ttl"].(string)

	if v, ok := usage["input_tokens"].(float64); ok {
		currentInput := int(v)
		if !hasCacheTokens && ((currentInput <= 1) || (estimatedInput > currentInput && estimatedInput > 1)) {
			usage["input_tokens"] = estimatedInput
			inputPatched = true
		}
	}

	if v, ok := usage["output_tokens"].(float64); ok {
		currentOutput := int(v)
		if currentOutput <= 1 || (estimatedOutput > currentOutput && estimatedOutput > 1) {
			usage["output_tokens"] = estimatedOutput
			outputPatched = true
		}
	}

	if enableLog {
		if inputPatched || outputPatched {
			log.Printf("ğŸ”¢ [Stream-Tokenè¡¥å…¨] %s: InputTokens=%vâ†’%v, OutputTokens=%vâ†’%v",
				location, originalInput, usage["input_tokens"], originalOutput, usage["output_tokens"])
		}
		log.Printf("ğŸ”¢ [Stream-Tokenç»Ÿè®¡] %s: InputTokens=%v, OutputTokens=%v, CacheCreationInputTokens=%.0f, CacheReadInputTokens=%.0f, CacheCreation5m=%.0f, CacheCreation1h=%.0f, CacheTTL=%s",
			location, usage["input_tokens"], usage["output_tokens"], cacheCreation, cacheRead, cacheCreation5m, cacheCreation1h, cacheTTL)
	}
}

// BuildStreamErrorEvent æ„å»ºæµé”™è¯¯ SSE äº‹ä»¶
func BuildStreamErrorEvent(err error) string {
	errorEvent := map[string]interface{}{
		"type": "error",
		"error": map[string]interface{}{
			"type":    "stream_error",
			"message": fmt.Sprintf("Stream processing error: %v", err),
		},
	}
	eventJSON, _ := json.Marshal(errorEvent)
	return fmt.Sprintf("event: error\ndata: %s\n\n", eventJSON)
}

// BuildUsageEvent æ„å»ºå¸¦ usage çš„ message_delta SSE äº‹ä»¶
func BuildUsageEvent(requestBody []byte, outputText string) string {
	inputTokens := utils.EstimateRequestTokens(requestBody)
	outputTokens := utils.EstimateTokens(outputText)

	event := map[string]interface{}{
		"type": "message_delta",
		"usage": map[string]int{
			"input_tokens":  inputTokens,
			"output_tokens": outputTokens,
		},
	}
	eventJSON, _ := json.Marshal(event)
	return fmt.Sprintf("event: message_delta\ndata: %s\n\n", eventJSON)
}

// IsMessageStopEvent æ£€æµ‹æ˜¯å¦ä¸º message_stop äº‹ä»¶
func IsMessageStopEvent(event string) bool {
	if strings.Contains(event, "event: message_stop") {
		return true
	}

	for _, line := range strings.Split(event, "\n") {
		if !strings.HasPrefix(line, "data: ") {
			continue
		}
		jsonStr := strings.TrimPrefix(line, "data: ")

		var data map[string]interface{}
		if err := json.Unmarshal([]byte(jsonStr), &data); err != nil {
			continue
		}

		if data["type"] == "message_stop" {
			return true
		}
	}
	return false
}

// IsMessageDeltaEvent æ£€æµ‹æ˜¯å¦ä¸º message_delta äº‹ä»¶
func IsMessageDeltaEvent(event string) bool {
	if strings.Contains(event, "event: message_delta") {
		return true
	}
	for _, line := range strings.Split(event, "\n") {
		if !strings.HasPrefix(line, "data: ") {
			continue
		}
		jsonStr := strings.TrimPrefix(line, "data: ")
		var data map[string]interface{}
		if err := json.Unmarshal([]byte(jsonStr), &data); err != nil {
			continue
		}
		if data["type"] == "message_delta" {
			return true
		}
	}
	return false
}

// ExtractTextFromEvent ä» SSE äº‹ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹
func ExtractTextFromEvent(event string, buf *bytes.Buffer) {
	for _, line := range strings.Split(event, "\n") {
		if !strings.HasPrefix(line, "data: ") {
			continue
		}
		jsonStr := strings.TrimPrefix(line, "data: ")

		var data map[string]interface{}
		if err := json.Unmarshal([]byte(jsonStr), &data); err != nil {
			continue
		}

		// Claude SSE: delta.text
		if delta, ok := data["delta"].(map[string]interface{}); ok {
			if text, ok := delta["text"].(string); ok {
				buf.WriteString(text)
			}
			if partialJSON, ok := delta["partial_json"].(string); ok {
				buf.WriteString(partialJSON)
			}
		}

		// content_block_start ä¸­çš„åˆå§‹æ–‡æœ¬
		if cb, ok := data["content_block"].(map[string]interface{}); ok {
			if text, ok := cb["text"].(string); ok {
				buf.WriteString(text)
			}
		}
	}
}
