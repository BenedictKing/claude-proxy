package handlers

import (
	"bytes"
	"encoding/json"
	"fmt"
	"io"
	"log"
	"net/http"
	"strings"
	"time"

	"github.com/BenedictKing/claude-proxy/internal/config"
	"github.com/BenedictKing/claude-proxy/internal/httpclient"
	"github.com/BenedictKing/claude-proxy/internal/middleware"
	"github.com/BenedictKing/claude-proxy/internal/providers"
	"github.com/BenedictKing/claude-proxy/internal/scheduler"
	"github.com/BenedictKing/claude-proxy/internal/types"
	"github.com/BenedictKing/claude-proxy/internal/utils"
	"github.com/gin-gonic/gin"
)

// ProxyHandler ä»£ç†å¤„ç†å™¨
// æ”¯æŒå¤šæ¸ é“è°ƒåº¦ï¼šå½“é…ç½®å¤šä¸ªæ¸ é“æ—¶è‡ªåŠ¨å¯ç”¨
func ProxyHandler(envCfg *config.EnvConfig, cfgManager *config.ConfigManager, channelScheduler *scheduler.ChannelScheduler) gin.HandlerFunc {
	return gin.HandlerFunc(func(c *gin.Context) {
		// å…ˆè¿›è¡Œè®¤è¯
		middleware.ProxyAuthMiddleware(envCfg)(c)
		if c.IsAborted() {
			return
		}

		startTime := time.Now()

		// è¯»å–åŸå§‹è¯·æ±‚ä½“ï¼ˆé™åˆ¶æœ€å¤§å¤§å°ï¼Œé€šè¿‡ç¯å¢ƒå˜é‡é…ç½®ï¼‰
		maxBodySize := envCfg.MaxRequestBodySize
		limitedReader := io.LimitReader(c.Request.Body, maxBodySize+1)
		bodyBytes, err := io.ReadAll(limitedReader)
		if err != nil {
			c.JSON(400, gin.H{"error": "Failed to read request body"})
			return
		}
		if int64(len(bodyBytes)) > maxBodySize {
			// æ’ç©ºå‰©ä½™è¯·æ±‚ä½“ï¼Œé¿å… keep-alive è¿æ¥æ±¡æŸ“
			io.Copy(io.Discard, c.Request.Body)
			c.JSON(413, gin.H{"error": fmt.Sprintf("Request body too large, maximum size is %d MB", maxBodySize/1024/1024)})
			return
		}
		// æ¢å¤è¯·æ±‚ä½“ä¾›åç»­ä½¿ç”¨
		c.Request.Body = io.NopCloser(bytes.NewReader(bodyBytes))

		// claudeReq å˜é‡ç”¨äºåˆ¤æ–­æ˜¯å¦æµå¼è¯·æ±‚å’Œæå– user_id
		var claudeReq types.ClaudeRequest
		if len(bodyBytes) > 0 {
			_ = json.Unmarshal(bodyBytes, &claudeReq)
		}

		// æå– user_id ç”¨äº Trace äº²å’Œæ€§
		userID := extractUserID(bodyBytes)

		// æ£€æŸ¥æ˜¯å¦ä¸ºå¤šæ¸ é“æ¨¡å¼
		isMultiChannel := channelScheduler.IsMultiChannelMode(false)

		if isMultiChannel {
			// å¤šæ¸ é“æ¨¡å¼ï¼šä½¿ç”¨è°ƒåº¦å™¨
			handleMultiChannelProxy(c, envCfg, cfgManager, channelScheduler, bodyBytes, claudeReq, userID, startTime)
		} else {
			// å•æ¸ é“æ¨¡å¼ï¼šä½¿ç”¨ç°æœ‰é€»è¾‘ï¼ˆä¹Ÿè®°å½•æŒ‡æ ‡ï¼‰
			handleSingleChannelProxy(c, envCfg, cfgManager, channelScheduler, bodyBytes, claudeReq, startTime)
		}
	})
}

// extractUserID ä»è¯·æ±‚ä½“ä¸­æå– user_id
func extractUserID(bodyBytes []byte) string {
	var req struct {
		Metadata struct {
			UserID string `json:"user_id"`
		} `json:"metadata"`
	}
	if err := json.Unmarshal(bodyBytes, &req); err == nil {
		return req.Metadata.UserID
	}
	return ""
}

// handleMultiChannelProxy å¤„ç†å¤šæ¸ é“ä»£ç†è¯·æ±‚
func handleMultiChannelProxy(
	c *gin.Context,
	envCfg *config.EnvConfig,
	cfgManager *config.ConfigManager,
	channelScheduler *scheduler.ChannelScheduler,
	bodyBytes []byte,
	claudeReq types.ClaudeRequest,
	userID string,
	startTime time.Time,
) {
	failedChannels := make(map[int]bool)
	var lastError error
	var lastFailoverError *struct {
		Status int
		Body   []byte
	}

	// è·å–æ´»è·ƒæ¸ é“æ•°é‡ä½œä¸ºæœ€å¤§é‡è¯•æ¬¡æ•°
	maxChannelAttempts := channelScheduler.GetActiveChannelCount(false)

	for channelAttempt := 0; channelAttempt < maxChannelAttempts; channelAttempt++ {
		// ä½¿ç”¨è°ƒåº¦å™¨é€‰æ‹©æ¸ é“
		selection, err := channelScheduler.SelectChannel(c.Request.Context(), userID, failedChannels, false)
		if err != nil {
			lastError = err
			break
		}

		upstream := selection.Upstream
		channelIndex := selection.ChannelIndex

		if envCfg.ShouldLog("info") {
			log.Printf("ğŸ¯ [å¤šæ¸ é“] é€‰æ‹©æ¸ é“: [%d] %s (åŸå› : %s, å°è¯• %d/%d)",
				channelIndex, upstream.Name, selection.Reason, channelAttempt+1, maxChannelAttempts)
		}

		// å°è¯•ä½¿ç”¨è¯¥æ¸ é“çš„æ‰€æœ‰ keyï¼Œè¿”å›æˆåŠŸä½¿ç”¨çš„ key
		success, successKey, failoverErr := tryChannelWithAllKeys(c, envCfg, cfgManager, channelScheduler, upstream, bodyBytes, claudeReq, startTime, false)

		if success {
			// è®°å½•æˆåŠŸçš„ keyï¼Œæ›´æ–° Trace äº²å’Œ
			if successKey != "" {
				channelScheduler.RecordSuccess(upstream.BaseURL, successKey, false)
			}
			channelScheduler.SetTraceAffinity(userID, channelIndex)
			return
		}

		// æ¸ é“æ‰€æœ‰ key éƒ½å¤±è´¥ï¼Œæ ‡è®°æ¸ é“å¤±è´¥
		failedChannels[channelIndex] = true

		if failoverErr != nil {
			lastFailoverError = failoverErr
			lastError = fmt.Errorf("æ¸ é“ [%d] %s å¤±è´¥", channelIndex, upstream.Name)
		}

		log.Printf("âš ï¸ [å¤šæ¸ é“] æ¸ é“ [%d] %s æ‰€æœ‰å¯†é’¥éƒ½å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæ¸ é“", channelIndex, upstream.Name)
	}

	// æ‰€æœ‰æ¸ é“éƒ½å¤±è´¥
	log.Printf("ğŸ’¥ [å¤šæ¸ é“] æ‰€æœ‰æ¸ é“éƒ½å¤±è´¥äº†")

	if lastFailoverError != nil {
		status := lastFailoverError.Status
		if status == 0 {
			status = 503
		}
		var errBody map[string]interface{}
		if err := json.Unmarshal(lastFailoverError.Body, &errBody); err == nil {
			c.JSON(status, errBody)
		} else {
			c.JSON(status, gin.H{"error": string(lastFailoverError.Body)})
		}
	} else {
		errMsg := "æ‰€æœ‰æ¸ é“éƒ½ä¸å¯ç”¨"
		if lastError != nil {
			errMsg = lastError.Error()
		}
		c.JSON(503, gin.H{
			"error":   "æ‰€æœ‰æ¸ é“éƒ½ä¸å¯ç”¨",
			"details": errMsg,
		})
	}
}

// tryChannelWithAllKeys å°è¯•ä½¿ç”¨æ¸ é“çš„æ‰€æœ‰å¯†é’¥
// è¿”å› (success bool, successKey string, lastFailoverError *struct{Status int; Body []byte})
func tryChannelWithAllKeys(
	c *gin.Context,
	envCfg *config.EnvConfig,
	cfgManager *config.ConfigManager,
	channelScheduler *scheduler.ChannelScheduler,
	upstream *config.UpstreamConfig,
	bodyBytes []byte,
	claudeReq types.ClaudeRequest,
	startTime time.Time,
	isResponses bool,
) (bool, string, *struct {
	Status int
	Body   []byte
}) {
	if len(upstream.APIKeys) == 0 {
		return false, "", nil
	}

	provider := providers.GetProvider(upstream.ServiceType)
	if provider == nil {
		return false, "", nil
	}

	// è·å–æŒ‡æ ‡ç®¡ç†å™¨ç”¨äºæ£€æŸ¥ç†”æ–­çŠ¶æ€
	metricsManager := channelScheduler.GetMessagesMetricsManager()
	if isResponses {
		metricsManager = channelScheduler.GetResponsesMetricsManager()
	}

	maxRetries := len(upstream.APIKeys)
	failedKeys := make(map[string]bool)
	var lastFailoverError *struct {
		Status int
		Body   []byte
	}
	deprioritizeCandidates := make(map[string]bool)

	for attempt := 0; attempt < maxRetries; attempt++ {
		// æ¢å¤è¯·æ±‚ä½“
		c.Request.Body = io.NopCloser(bytes.NewReader(bodyBytes))

		apiKey, err := cfgManager.GetNextAPIKey(upstream, failedKeys)
		if err != nil {
			break
		}

		// æ£€æŸ¥è¯¥ Key æ˜¯å¦å¤„äºç†”æ–­çŠ¶æ€ï¼Œè·³è¿‡ç†”æ–­çš„ Key
		if metricsManager.ShouldSuspendKey(upstream.BaseURL, apiKey) {
			failedKeys[apiKey] = true
			log.Printf("âš¡ è·³è¿‡ç†”æ–­ä¸­çš„ Key: %s", utils.MaskAPIKey(apiKey))
			continue
		}

		if envCfg.ShouldLog("info") {
			log.Printf("ğŸ”‘ ä½¿ç”¨APIå¯†é’¥: %s (å°è¯• %d/%d)", utils.MaskAPIKey(apiKey), attempt+1, maxRetries)
		}

		// è½¬æ¢è¯·æ±‚
		providerReq, _, err := provider.ConvertToProviderRequest(c, upstream, apiKey)
		if err != nil {
			failedKeys[apiKey] = true
			// è®°å½•è¯¥ key å¤±è´¥
			channelScheduler.RecordFailure(upstream.BaseURL, apiKey, isResponses)
			continue
		}

		// å‘é€è¯·æ±‚
		resp, err := sendRequest(providerReq, upstream, envCfg, claudeReq.Stream)
		if err != nil {
			failedKeys[apiKey] = true
			cfgManager.MarkKeyAsFailed(apiKey)
			// è®°å½•è¯¥ key å¤±è´¥
			channelScheduler.RecordFailure(upstream.BaseURL, apiKey, isResponses)
			log.Printf("âš ï¸ APIå¯†é’¥å¤±è´¥: %v", err)
			continue
		}

		// æ£€æŸ¥å“åº”çŠ¶æ€
		if resp.StatusCode < 200 || resp.StatusCode >= 300 {
			respBodyBytes, _ := io.ReadAll(resp.Body)
			resp.Body.Close()
			respBodyBytes = utils.DecompressGzipIfNeeded(resp, respBodyBytes)

			shouldFailover, isQuotaRelated := shouldRetryWithNextKey(resp.StatusCode, respBodyBytes)
			if shouldFailover {
				failedKeys[apiKey] = true
				cfgManager.MarkKeyAsFailed(apiKey)
				// è®°å½•è¯¥ key å¤±è´¥
				channelScheduler.RecordFailure(upstream.BaseURL, apiKey, isResponses)
				log.Printf("âš ï¸ APIå¯†é’¥å¤±è´¥ (çŠ¶æ€: %d)ï¼Œå°è¯•ä¸‹ä¸€ä¸ªå¯†é’¥", resp.StatusCode)

				lastFailoverError = &struct {
					Status int
					Body   []byte
				}{
					Status: resp.StatusCode,
					Body:   respBodyBytes,
				}

				if isQuotaRelated {
					deprioritizeCandidates[apiKey] = true
				}
				continue
			}

			// é failover é”™è¯¯ï¼Œç›´æ¥è¿”å›ï¼ˆè¯·æ±‚å·²å¤„ç†ä½†ä¸ç®—æˆåŠŸï¼‰
			c.Data(resp.StatusCode, "application/json", respBodyBytes)
			return true, "", nil // è¿”å› true è¡¨ç¤ºè¯·æ±‚å·²å¤„ç†ï¼Œä½† successKey ä¸ºç©ºè¡¨ç¤ºä¸è®°å½•æˆåŠŸ
		}

		// å¤„ç†æˆåŠŸå“åº”
		if len(deprioritizeCandidates) > 0 {
			for key := range deprioritizeCandidates {
				if err := cfgManager.DeprioritizeAPIKey(key); err != nil {
					log.Printf("âš ï¸ å¯†é’¥é™çº§å¤±è´¥: %v", err)
				}
			}
		}

		if claudeReq.Stream {
			handleStreamResponse(c, resp, provider, envCfg, startTime, upstream, bodyBytes)
		} else {
			handleNormalResponse(c, resp, provider, envCfg, startTime, bodyBytes)
		}
		return true, apiKey, nil
	}

	return false, "", lastFailoverError
}

// handleSingleChannelProxy å¤„ç†å•æ¸ é“ä»£ç†è¯·æ±‚ï¼ˆç°æœ‰é€»è¾‘ï¼‰
func handleSingleChannelProxy(
	c *gin.Context,
	envCfg *config.EnvConfig,
	cfgManager *config.ConfigManager,
	channelScheduler *scheduler.ChannelScheduler,
	bodyBytes []byte,
	claudeReq types.ClaudeRequest,
	startTime time.Time,
) {
	// è·å–å½“å‰ä¸Šæ¸¸é…ç½®
	upstream, err := cfgManager.GetCurrentUpstream()
	if err != nil {
		c.JSON(503, gin.H{
			"error": "æœªé…ç½®ä»»ä½•æ¸ é“ï¼Œè¯·å…ˆåœ¨ç®¡ç†ç•Œé¢æ·»åŠ æ¸ é“",
			"code":  "NO_UPSTREAM",
		})
		return
	}

	if len(upstream.APIKeys) == 0 {
		c.JSON(503, gin.H{
			"error": fmt.Sprintf("å½“å‰æ¸ é“ \"%s\" æœªé…ç½®APIå¯†é’¥", upstream.Name),
			"code":  "NO_API_KEYS",
		})
		return
	}

	// è·å–æä¾›å•†
	provider := providers.GetProvider(upstream.ServiceType)
	if provider == nil {
		c.JSON(400, gin.H{"error": "Unsupported service type"})
		return
	}

	// å®ç° failover é‡è¯•é€»è¾‘
	maxRetries := len(upstream.APIKeys)
	failedKeys := make(map[string]bool)
	var lastError error
	var lastOriginalBodyBytes []byte
	var lastFailoverError *struct {
		Status int
		Body   []byte
	}
	deprioritizeCandidates := make(map[string]bool)

	// è·å–æŒ‡æ ‡ç®¡ç†å™¨ç”¨äºæ£€æŸ¥ç†”æ–­çŠ¶æ€
	metricsManager := channelScheduler.GetMessagesMetricsManager()

	for attempt := 0; attempt < maxRetries; attempt++ {
		// æ¢å¤è¯·æ±‚ä½“
		c.Request.Body = io.NopCloser(bytes.NewReader(bodyBytes))

		apiKey, err := cfgManager.GetNextAPIKey(upstream, failedKeys)
		if err != nil {
			lastError = err
			break
		}

		// æ£€æŸ¥è¯¥ Key æ˜¯å¦å¤„äºç†”æ–­çŠ¶æ€ï¼Œè·³è¿‡ç†”æ–­çš„ Key
		if metricsManager.ShouldSuspendKey(upstream.BaseURL, apiKey) {
			failedKeys[apiKey] = true
			log.Printf("âš¡ è·³è¿‡ç†”æ–­ä¸­çš„ Key: %s", utils.MaskAPIKey(apiKey))
			continue
		}

		if envCfg.ShouldLog("info") {
			log.Printf("ğŸ¯ ä½¿ç”¨ä¸Šæ¸¸: %s - %s (å°è¯• %d/%d)", upstream.Name, upstream.BaseURL, attempt+1, maxRetries)
			log.Printf("ğŸ”‘ ä½¿ç”¨APIå¯†é’¥: %s", utils.MaskAPIKey(apiKey))
		}

		// è½¬æ¢è¯·æ±‚
		providerReq, originalBodyBytes, err := provider.ConvertToProviderRequest(c, upstream, apiKey)
		if err != nil {
			lastError = err
			failedKeys[apiKey] = true
			channelScheduler.RecordFailure(upstream.BaseURL, apiKey, false)
			if originalBodyBytes != nil {
				lastOriginalBodyBytes = originalBodyBytes
			}
			continue
		}
		lastOriginalBodyBytes = originalBodyBytes

		// è¯·æ±‚æ—¥å¿—è®°å½•
		if envCfg.EnableRequestLogs {
			log.Printf("ğŸ“¥ æ”¶åˆ°è¯·æ±‚: %s %s", c.Request.Method, c.Request.URL.Path)
			if envCfg.IsDevelopment() {
				logBody := lastOriginalBodyBytes
				if len(logBody) == 0 && c.Request.Body != nil {
					bodyFromContext, _ := io.ReadAll(c.Request.Body)
					c.Request.Body = io.NopCloser(bytes.NewReader(bodyFromContext))
					logBody = bodyFromContext
				}
				formattedBody := utils.FormatJSONBytesForLog(logBody, 500)
				log.Printf("ğŸ“„ åŸå§‹è¯·æ±‚ä½“:\n%s", formattedBody)

				sanitizedHeaders := make(map[string]string)
				for key, values := range c.Request.Header {
					if len(values) > 0 {
						sanitizedHeaders[key] = values[0]
					}
				}
				maskedHeaders := utils.MaskSensitiveHeaders(sanitizedHeaders)
				headersJSON, _ := json.MarshalIndent(maskedHeaders, "", "  ")
				log.Printf("ğŸ“¥ åŸå§‹è¯·æ±‚å¤´:\n%s", string(headersJSON))
			}
		}

		// å‘é€è¯·æ±‚
		resp, err := sendRequest(providerReq, upstream, envCfg, claudeReq.Stream)
		if err != nil {
			lastError = err
			failedKeys[apiKey] = true
			cfgManager.MarkKeyAsFailed(apiKey)
			channelScheduler.RecordFailure(upstream.BaseURL, apiKey, false)
			log.Printf("âš ï¸ APIå¯†é’¥å¤±è´¥: %v", err)
			continue
		}

		// æ£€æŸ¥å“åº”çŠ¶æ€
		if resp.StatusCode < 200 || resp.StatusCode >= 300 {
			respBodyBytes, _ := io.ReadAll(resp.Body)
			resp.Body.Close()
			respBodyBytes = utils.DecompressGzipIfNeeded(resp, respBodyBytes)

			shouldFailover, isQuotaRelated := shouldRetryWithNextKey(resp.StatusCode, respBodyBytes)
			if shouldFailover {
				lastError = fmt.Errorf("ä¸Šæ¸¸é”™è¯¯: %d", resp.StatusCode)
				failedKeys[apiKey] = true
				cfgManager.MarkKeyAsFailed(apiKey)
				channelScheduler.RecordFailure(upstream.BaseURL, apiKey, false)

				log.Printf("âš ï¸ APIå¯†é’¥å¤±è´¥ (çŠ¶æ€: %d)ï¼Œå°è¯•ä¸‹ä¸€ä¸ªå¯†é’¥", resp.StatusCode)
				if envCfg.EnableResponseLogs && envCfg.IsDevelopment() {
					formattedBody := utils.FormatJSONBytesForLog(respBodyBytes, 500)
					log.Printf("ğŸ“¦ å¤±è´¥åŸå› :\n%s", formattedBody)
				} else if envCfg.EnableResponseLogs {
					log.Printf("å¤±è´¥åŸå› : %s", string(respBodyBytes))
				}

				lastFailoverError = &struct {
					Status int
					Body   []byte
				}{
					Status: resp.StatusCode,
					Body:   respBodyBytes,
				}

				if isQuotaRelated {
					deprioritizeCandidates[apiKey] = true
				}
				continue
			}

			// é failover é”™è¯¯
			if envCfg.EnableResponseLogs {
				log.Printf("âš ï¸ ä¸Šæ¸¸è¿”å›é”™è¯¯: %d", resp.StatusCode)
				if envCfg.IsDevelopment() {
					formattedBody := utils.FormatJSONBytesForLog(respBodyBytes, 500)
					log.Printf("ğŸ“¦ é”™è¯¯å“åº”ä½“:\n%s", formattedBody)

					respHeaders := make(map[string]string)
					for key, values := range resp.Header {
						if len(values) > 0 {
							respHeaders[key] = values[0]
						}
					}
					respHeadersJSON, _ := json.MarshalIndent(respHeaders, "", "  ")
					log.Printf("ğŸ“‹ é”™è¯¯å“åº”å¤´:\n%s", string(respHeadersJSON))
				}
			}
			c.Data(resp.StatusCode, "application/json", respBodyBytes)
			return
		}

		// å¤„ç†æˆåŠŸå“åº”
		channelScheduler.RecordSuccess(upstream.BaseURL, apiKey, false)

		if len(deprioritizeCandidates) > 0 {
			for key := range deprioritizeCandidates {
				if err := cfgManager.DeprioritizeAPIKey(key); err != nil {
					log.Printf("âš ï¸ å¯†é’¥é™çº§å¤±è´¥: %v", err)
				}
			}
		}

		if claudeReq.Stream {
			handleStreamResponse(c, resp, provider, envCfg, startTime, upstream, bodyBytes)
		} else {
			handleNormalResponse(c, resp, provider, envCfg, startTime, bodyBytes)
		}
		return
	}

	// æ‰€æœ‰å¯†é’¥éƒ½å¤±è´¥äº†
	log.Printf("ğŸ’¥ æ‰€æœ‰APIå¯†é’¥éƒ½å¤±è´¥äº†")

	if lastFailoverError != nil {
		status := lastFailoverError.Status
		if status == 0 {
			status = 500
		}
		var errBody map[string]interface{}
		if err := json.Unmarshal(lastFailoverError.Body, &errBody); err == nil {
			c.JSON(status, errBody)
		} else {
			c.JSON(status, gin.H{"error": string(lastFailoverError.Body)})
		}
	} else {
		errMsg := "æœªçŸ¥é”™è¯¯"
		if lastError != nil {
			errMsg = lastError.Error()
		}
		c.JSON(500, gin.H{
			"error":   "æ‰€æœ‰ä¸Šæ¸¸APIå¯†é’¥éƒ½ä¸å¯ç”¨",
			"details": errMsg,
		})
	}
}

// sendRequest å‘é€HTTPè¯·æ±‚
func sendRequest(req *http.Request, upstream *config.UpstreamConfig, envCfg *config.EnvConfig, isStream bool) (*http.Response, error) {
	// ä½¿ç”¨å…¨å±€å®¢æˆ·ç«¯ç®¡ç†å™¨
	clientManager := httpclient.GetManager()

	var client *http.Client
	if isStream {
		// æµå¼è¯·æ±‚ï¼šä½¿ç”¨æ— è¶…æ—¶çš„å®¢æˆ·ç«¯
		client = clientManager.GetStreamClient(upstream.InsecureSkipVerify)
	} else {
		// æ™®é€šè¯·æ±‚ï¼šä½¿ç”¨æœ‰è¶…æ—¶çš„å®¢æˆ·ç«¯
		timeout := time.Duration(envCfg.RequestTimeout) * time.Millisecond
		client = clientManager.GetStandardClient(timeout, upstream.InsecureSkipVerify)
	}

	if upstream.InsecureSkipVerify && envCfg.EnableRequestLogs {
		log.Printf("âš ï¸ æ­£åœ¨è·³è¿‡å¯¹ %s çš„TLSè¯ä¹¦éªŒè¯", req.URL.String())
	}

	if envCfg.EnableRequestLogs {
		log.Printf("ğŸŒ å®é™…è¯·æ±‚URL: %s", req.URL.String())
		log.Printf("ğŸ“¤ è¯·æ±‚æ–¹æ³•: %s", req.Method)
		if envCfg.IsDevelopment() {
			// å¯¹è¯·æ±‚å¤´åšæ•æ„Ÿä¿¡æ¯è„±æ•
			reqHeaders := make(map[string]string)
			for key, values := range req.Header {
				if len(values) > 0 {
					reqHeaders[key] = values[0]
				}
			}
			maskedReqHeaders := utils.MaskSensitiveHeaders(reqHeaders)
			reqHeadersJSON, _ := json.MarshalIndent(maskedReqHeaders, "", "  ")
			log.Printf("ğŸ“‹ å®é™…è¯·æ±‚å¤´:\n%s", string(reqHeadersJSON))

			if req.Body != nil {
				// è¯»å–è¯·æ±‚ä½“ç”¨äºæ—¥å¿—
				bodyBytes, err := io.ReadAll(req.Body)
				if err == nil {
					// æ¢å¤è¯·æ±‚ä½“
					req.Body = io.NopCloser(bytes.NewReader(bodyBytes))

					// ä½¿ç”¨æ™ºèƒ½æˆªæ–­å’Œç®€åŒ–å‡½æ•°ï¼ˆä¸TSç‰ˆæœ¬å¯¹é½ï¼‰
					formattedBody := utils.FormatJSONBytesForLog(bodyBytes, 500)
					log.Printf("ğŸ“¦ å®é™…è¯·æ±‚ä½“:\n%s", formattedBody)
				}
			}
		}
	}

	return client.Do(req)
}

// handleNormalResponse å¤„ç†éæµå¼å“åº”
func handleNormalResponse(c *gin.Context, resp *http.Response, provider providers.Provider, envCfg *config.EnvConfig, startTime time.Time, requestBody []byte) {
	defer resp.Body.Close()

	bodyBytes, err := io.ReadAll(resp.Body)
	if err != nil {
		c.JSON(500, gin.H{"error": "Failed to read response"})
		return
	}

	if envCfg.EnableResponseLogs {
		responseTime := time.Since(startTime).Milliseconds()
		log.Printf("â±ï¸ å“åº”å®Œæˆ: %dms, çŠ¶æ€: %d", responseTime, resp.StatusCode)
		if envCfg.IsDevelopment() {
			// å“åº”å¤´(ä¸éœ€è¦è„±æ•)
			respHeaders := make(map[string]string)
			for key, values := range resp.Header {
				if len(values) > 0 {
					respHeaders[key] = values[0]
				}
			}
			respHeadersJSON, _ := json.MarshalIndent(respHeaders, "", "  ")
			log.Printf("ğŸ“‹ å“åº”å¤´:\n%s", string(respHeadersJSON))

			// ä½¿ç”¨æ™ºèƒ½æˆªæ–­ï¼ˆä¸TSç‰ˆæœ¬å¯¹é½ï¼‰
			formattedBody := utils.FormatJSONBytesForLog(bodyBytes, 500)
			log.Printf("ğŸ“¦ å“åº”ä½“:\n%s", formattedBody)
		}
	}

	providerResp := &types.ProviderResponse{
		StatusCode: resp.StatusCode,
		Headers:    resp.Header,
		Body:       bodyBytes,
		Stream:     false,
	}

	claudeResp, err := provider.ConvertToClaudeResponse(providerResp)
	if err != nil {
		c.JSON(500, gin.H{"error": "Failed to convert response"})
		return
	}

	// å¦‚æœä¸Šæ¸¸æ²¡æœ‰è¿”å› Usageï¼Œæœ¬åœ°ä¼°ç®—
	// å¦‚æœ input_tokens ä¸º 0 æˆ– 1ï¼ˆè™šå‡å€¼ï¼‰ï¼Œä¹Ÿéœ€è¦è¡¥å…¨
	// ä½†å¦‚æœæœ‰ cache_creation_input_tokens æˆ– cache_read_input_tokensï¼Œåˆ™ input_tokens ä¸º 0/1 æ˜¯æ­£å¸¸çš„
	if claudeResp.Usage == nil {
		estimatedInput := utils.EstimateRequestTokens(requestBody)
		estimatedOutput := utils.EstimateResponseTokens(claudeResp.Content)
		claudeResp.Usage = &types.Usage{
			InputTokens:  estimatedInput,
			OutputTokens: estimatedOutput,
		}
		if envCfg.EnableResponseLogs {
			log.Printf("ğŸ”¢ [Tokenè¡¥å…¨] ä¸Šæ¸¸æ— Usage, æœ¬åœ°ä¼°ç®—: input=%d, output=%d", estimatedInput, estimatedOutput)
		}
	} else {
		originalInput := claudeResp.Usage.InputTokens
		originalOutput := claudeResp.Usage.OutputTokens
		patched := false

		// æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜ tokenï¼ˆå¦‚æœæœ‰ï¼Œinput_tokens ä¸º 0/1 æ˜¯æ­£å¸¸çš„ï¼‰
		hasCacheTokens := claudeResp.Usage.CacheCreationInputTokens > 0 || claudeResp.Usage.CacheReadInputTokens > 0

		// åªæœ‰åœ¨æ²¡æœ‰ç¼“å­˜ token çš„æƒ…å†µä¸‹æ‰è¡¥å…¨ input_tokens
		if claudeResp.Usage.InputTokens <= 1 && !hasCacheTokens {
			claudeResp.Usage.InputTokens = utils.EstimateRequestTokens(requestBody)
			patched = true
		}
		if claudeResp.Usage.OutputTokens <= 1 {
			claudeResp.Usage.OutputTokens = utils.EstimateResponseTokens(claudeResp.Content)
			patched = true
		}
		if envCfg.EnableResponseLogs {
			if patched {
				log.Printf("ğŸ”¢ [Tokenè¡¥å…¨] è™šå‡å€¼: InputTokens=%dâ†’%d, OutputTokens=%dâ†’%d",
					originalInput, claudeResp.Usage.InputTokens, originalOutput, claudeResp.Usage.OutputTokens)
			}
			// è®°å½•å®Œæ•´çš„ token ä¿¡æ¯
			log.Printf("ğŸ”¢ [Tokenç»Ÿè®¡] InputTokens=%d, OutputTokens=%d, CacheCreationInputTokens=%d, CacheReadInputTokens=%d, PromptTokens=%d, CompletionTokens=%d",
				claudeResp.Usage.InputTokens, claudeResp.Usage.OutputTokens,
				claudeResp.Usage.CacheCreationInputTokens, claudeResp.Usage.CacheReadInputTokens,
				claudeResp.Usage.PromptTokens, claudeResp.Usage.CompletionTokens)
		}
	}

	// ç›‘å¬å®¢æˆ·ç«¯æ–­å¼€è¿æ¥
	ctx := c.Request.Context()
	go func() {
		<-ctx.Done()
		// æ£€æŸ¥å“åº”æ˜¯å¦å·²å®Œæˆ
		if !c.Writer.Written() {
			if envCfg.EnableResponseLogs {
				responseTime := time.Since(startTime).Milliseconds()
				log.Printf("â±ï¸ å“åº”ä¸­æ–­: %dms, çŠ¶æ€: %d", responseTime, resp.StatusCode)
			}
		}
	}()

	// è½¬å‘ä¸Šæ¸¸å“åº”å¤´åˆ°å®¢æˆ·ç«¯ï¼ˆé€æ˜ä»£ç†ï¼‰
	utils.ForwardResponseHeaders(resp.Header, c.Writer)

	c.JSON(200, claudeResp)

	// å“åº”å®Œæˆåè®°å½•
	if envCfg.EnableResponseLogs {
		responseTime := time.Since(startTime).Milliseconds()
		log.Printf("â±ï¸ å“åº”å‘é€å®Œæˆ: %dms, çŠ¶æ€: %d", responseTime, resp.StatusCode)
	}
}

// handleStreamResponse å¤„ç†æµå¼å“åº”
func handleStreamResponse(c *gin.Context, resp *http.Response, provider providers.Provider, envCfg *config.EnvConfig, startTime time.Time, upstream *config.UpstreamConfig, requestBody []byte) {
	defer resp.Body.Close()

	eventChan, errChan, err := provider.HandleStreamResponse(resp.Body)
	if err != nil {
		c.JSON(500, gin.H{"error": "Failed to handle stream response"})
		return
	}

	// è®¾ç½®å“åº”å¤´
	setupStreamHeaders(c, resp)

	w := c.Writer
	flusher, ok := w.(http.Flusher)
	if !ok {
		log.Printf("âš ï¸ ResponseWriterä¸æ”¯æŒFlushæ¥å£")
		return
	}
	flusher.Flush()

	// åˆå§‹åŒ–æµå¤„ç†ä¸Šä¸‹æ–‡
	ctx := newStreamContext(envCfg, upstream)

	// äº‹ä»¶å¾ªç¯
	processStreamEvents(c, w, flusher, eventChan, errChan, ctx, envCfg, startTime, requestBody)
}

// setupStreamHeaders è®¾ç½®æµå¼å“åº”å¤´
func setupStreamHeaders(c *gin.Context, resp *http.Response) {
	utils.ForwardResponseHeaders(resp.Header, c.Writer)
	c.Header("Content-Type", "text/event-stream")
	c.Header("Cache-Control", "no-cache")
	c.Header("Connection", "keep-alive")
	c.Header("X-Accel-Buffering", "no")
	c.Status(200)
}

// streamContext æµå¤„ç†ä¸Šä¸‹æ–‡
type streamContext struct {
	logBuffer        bytes.Buffer
	outputTextBuffer bytes.Buffer
	synthesizer      *utils.StreamSynthesizer
	loggingEnabled   bool
	clientGone       bool
	hasUsage         bool
	needTokenPatch   bool
	// ç´¯ç§¯çš„ token ç»Ÿè®¡ï¼ˆä»æµäº‹ä»¶ä¸­æ”¶é›†ï¼Œå€Ÿé‰´ new-api çš„è®¾è®¡ï¼‰
	// message_start: è·å– input_tokens å’Œ cache tokens
	// message_delta: è·å–æœ€ç»ˆçš„ output_tokensï¼Œå¦‚æœ input_tokens > 0 åˆ™æ›´æ–°
	collectedUsage collectedUsageData
}

func newStreamContext(envCfg *config.EnvConfig, upstream *config.UpstreamConfig) *streamContext {
	ctx := &streamContext{
		loggingEnabled: envCfg.IsDevelopment() && envCfg.EnableResponseLogs,
	}
	if ctx.loggingEnabled {
		// æ‰€æœ‰ Provider çš„ HandleStreamResponse éƒ½ä¼šå°†å“åº”è½¬æ¢ä¸º Claude SSE æ ¼å¼
		// å› æ­¤æ—¥å¿—åˆæˆå™¨åº”è¯¥ä½¿ç”¨ "claude" ç±»å‹æ¥è§£æè½¬æ¢åçš„äº‹ä»¶
		ctx.synthesizer = utils.NewStreamSynthesizer("claude")
	}
	return ctx
}

// processStreamEvents å¤„ç†æµäº‹ä»¶å¾ªç¯
func processStreamEvents(c *gin.Context, w gin.ResponseWriter, flusher http.Flusher, eventChan <-chan string, errChan <-chan error, ctx *streamContext, envCfg *config.EnvConfig, startTime time.Time, requestBody []byte) {
	for {
		select {
		case event, ok := <-eventChan:
			if !ok {
				logStreamCompletion(ctx, envCfg, startTime)
				return
			}
			processStreamEvent(c, w, flusher, event, ctx, envCfg, requestBody)

		case err, ok := <-errChan:
			if !ok {
				continue
			}
			if err != nil {
				log.Printf("ğŸ’¥ æµå¼ä¼ è¾“é”™è¯¯: %v", err)
				logPartialResponse(ctx, envCfg)
				return
			}
		}
	}
}

// processStreamEvent å¤„ç†å•ä¸ªæµäº‹ä»¶
func processStreamEvent(c *gin.Context, w gin.ResponseWriter, flusher http.Flusher, event string, ctx *streamContext, envCfg *config.EnvConfig, requestBody []byte) {
	// æå–æ–‡æœ¬ç”¨äºä¼°ç®— tokenï¼ˆå¿…é¡»åœ¨æ£€æµ‹ usage ä¹‹å‰ï¼Œç¡®ä¿ç´¯ç§¯å†…å®¹ï¼‰
	extractTextFromEvent(event, &ctx.outputTextBuffer)

	// æ£€æµ‹å¹¶æ”¶é›† usageï¼ˆå€Ÿé‰´ new-api çš„è®¾è®¡ï¼ŒæŒç»­ä»æµäº‹ä»¶ä¸­æ”¶é›† token ç»Ÿè®¡ï¼‰
	// message_start: è·å– input_tokens å’Œ cache tokens
	// message_delta: è·å–æœ€ç»ˆçš„ output_tokensï¼Œå¦‚æœ input_tokens > 0 åˆ™æ›´æ–°
	hasUsage, needPatch, usageData := checkEventUsageStatus(event, envCfg.EnableResponseLogs && envCfg.ShouldLog("debug"))
	if hasUsage {
		// é¦–æ¬¡æ£€æµ‹åˆ° usage
		if !ctx.hasUsage {
			ctx.hasUsage = true
			ctx.needTokenPatch = needPatch
			if envCfg.EnableResponseLogs && envCfg.ShouldLog("debug") && needPatch && !isMessageDeltaEvent(event) {
				log.Printf("ğŸ”¢ [Stream-Token] æ£€æµ‹åˆ°è™šå‡å€¼, å»¶è¿Ÿåˆ°æµç»“æŸä¿®è¡¥")
			}
		}
		// ç´¯ç§¯æ”¶é›† usage æ•°æ®
		// InputTokens: å–æœ€å¤§å€¼ï¼ˆé¿å…ä¸­é—´æ›´æ–°çš„çœŸå®å€¼è¢«æœ€ç»ˆäº‹ä»¶çš„æ—§å€¼è¦†ç›–ï¼‰
		// OutputTokens: å–æœ€å¤§å€¼ï¼ˆæœ€ç»ˆäº‹ä»¶çš„ output_tokens é€šå¸¸æ˜¯æœ€å‡†ç¡®çš„ï¼‰
		if usageData.InputTokens > ctx.collectedUsage.InputTokens {
			ctx.collectedUsage.InputTokens = usageData.InputTokens
		}
		if usageData.OutputTokens > ctx.collectedUsage.OutputTokens {
			ctx.collectedUsage.OutputTokens = usageData.OutputTokens
		}
		if usageData.CacheCreationInputTokens > 0 {
			ctx.collectedUsage.CacheCreationInputTokens = usageData.CacheCreationInputTokens
		}
		if usageData.CacheReadInputTokens > 0 {
			ctx.collectedUsage.CacheReadInputTokens = usageData.CacheReadInputTokens
		}
	}

	// æ—¥å¿—ç¼“å­˜
	if ctx.loggingEnabled {
		ctx.logBuffer.WriteString(event)
		if ctx.synthesizer != nil {
			for _, line := range strings.Split(event, "\n") {
				ctx.synthesizer.ProcessLine(line)
			}
		}
	}

	// åœ¨ message_stop å‰æ³¨å…¥ usageï¼ˆä¸Šæ¸¸å®Œå…¨æ²¡æœ‰ usage çš„æƒ…å†µï¼‰
	if !ctx.hasUsage && !ctx.clientGone && isMessageStopEvent(event) {
		usageEvent := buildUsageEvent(requestBody, ctx.outputTextBuffer.String())
		if envCfg.EnableResponseLogs && envCfg.ShouldLog("debug") {
			log.Printf("ğŸ”¢ [Stream-Tokenæ³¨å…¥] ä¸Šæ¸¸æ— usage, æ³¨å…¥æœ¬åœ°ä¼°ç®—äº‹ä»¶")
		}
		w.Write([]byte(usageEvent))
		flusher.Flush()
		ctx.hasUsage = true
	}

	// ä¿®è¡¥ tokenï¼ˆåœ¨ message_delta æˆ– message_stop æ—¶ä¿®è¡¥ï¼Œç¡®ä¿å†…å®¹å·²å®Œæ•´ç´¯ç§¯ï¼‰
	eventToSend := event
	if ctx.needTokenPatch && hasEventWithUsage(event) {
		// åªåœ¨æµç»“æŸäº‹ä»¶ï¼ˆmessage_delta æˆ– message_stopï¼‰æ—¶ä¿®è¡¥
		if isMessageDeltaEvent(event) || isMessageStopEvent(event) {
			// ä¼˜å…ˆä½¿ç”¨æ”¶é›†åˆ°çš„çœŸå® token å€¼ï¼Œå¦åˆ™ä½¿ç”¨ä¼°ç®—å€¼ï¼ˆå€Ÿé‰´ new-api çš„å®¹é”™è®¾è®¡ï¼‰
			inputTokens := ctx.collectedUsage.InputTokens
			if inputTokens == 0 {
				inputTokens = utils.EstimateRequestTokens(requestBody)
			}
			outputTokens := ctx.collectedUsage.OutputTokens
			if outputTokens == 0 {
				outputTokens = utils.EstimateTokens(ctx.outputTextBuffer.String())
			}
			// ä¼ é€’å·²æ”¶é›†çš„ç¼“å­˜ token ä¿¡æ¯ï¼Œé¿å…ä»æœ€ç»ˆäº‹ä»¶ä¸­è¯»å–ï¼ˆæœ€ç»ˆäº‹ä»¶é€šå¸¸ä¸å«ç¼“å­˜å­—æ®µï¼‰
			hasCacheTokens := ctx.collectedUsage.CacheCreationInputTokens > 0 || ctx.collectedUsage.CacheReadInputTokens > 0
			eventToSend = patchTokensInEvent(event, inputTokens, outputTokens, hasCacheTokens, envCfg.EnableResponseLogs && envCfg.ShouldLog("debug"))
			ctx.needTokenPatch = false
		}
	}

	// è½¬å‘ç»™å®¢æˆ·ç«¯
	if !ctx.clientGone {
		if _, err := w.Write([]byte(eventToSend)); err != nil {
			ctx.clientGone = true
			if !isClientDisconnectError(err) {
				log.Printf("âš ï¸ æµå¼ä¼ è¾“å†™å…¥é”™è¯¯: %v", err)
			} else if envCfg.ShouldLog("info") {
				log.Printf("â„¹ï¸ å®¢æˆ·ç«¯ä¸­æ–­è¿æ¥ (æ­£å¸¸è¡Œä¸º)ï¼Œç»§ç»­æ¥æ”¶ä¸Šæ¸¸æ•°æ®...")
			}
		} else {
			flusher.Flush()
		}
	}
}

// isClientDisconnectError åˆ¤æ–­æ˜¯å¦ä¸ºå®¢æˆ·ç«¯æ–­å¼€è¿æ¥é”™è¯¯
func isClientDisconnectError(err error) bool {
	msg := err.Error()
	return strings.Contains(msg, "broken pipe") || strings.Contains(msg, "connection reset")
}

// logStreamCompletion è®°å½•æµå®Œæˆæ—¥å¿—
func logStreamCompletion(ctx *streamContext, envCfg *config.EnvConfig, startTime time.Time) {
	if !envCfg.EnableResponseLogs {
		return
	}
	log.Printf("â±ï¸ æµå¼å“åº”å®Œæˆ: %dms", time.Since(startTime).Milliseconds())

	if envCfg.IsDevelopment() {
		logSynthesizedContent(ctx)
	}
}

// logPartialResponse è®°å½•éƒ¨åˆ†å“åº”æ—¥å¿—
func logPartialResponse(ctx *streamContext, envCfg *config.EnvConfig) {
	if envCfg.EnableResponseLogs && envCfg.IsDevelopment() {
		logSynthesizedContent(ctx)
	}
}

// logSynthesizedContent è®°å½•åˆæˆå†…å®¹
func logSynthesizedContent(ctx *streamContext) {
	if ctx.synthesizer != nil {
		content := ctx.synthesizer.GetSynthesizedContent()
		if content != "" && !ctx.synthesizer.IsParseFailed() {
			log.Printf("ğŸ›°ï¸  ä¸Šæ¸¸æµå¼å“åº”åˆæˆå†…å®¹:\n%s", strings.TrimSpace(content))
			return
		}
	}
	if ctx.logBuffer.Len() > 0 {
		log.Printf("ğŸ›°ï¸  ä¸Šæ¸¸æµå¼å“åº”åŸå§‹å†…å®¹:\n%s", ctx.logBuffer.String())
	}
}

// shouldRetryWithNextKey åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨ä¸‹ä¸€ä¸ªå¯†é’¥é‡è¯•
// è¿”å›: (shouldFailover bool, isQuotaRelated bool)
//
// HTTP çŠ¶æ€ç åˆ†ç±»ç­–ç•¥ï¼š
//   - 4xx å®¢æˆ·ç«¯é”™è¯¯ï¼šéƒ¨åˆ†åº”è§¦å‘ failoverï¼ˆå¯†é’¥/é…é¢é—®é¢˜ï¼‰
//   - 5xx æœåŠ¡ç«¯é”™è¯¯ï¼šåº”è§¦å‘ failoverï¼ˆä¸Šæ¸¸ä¸´æ—¶æ•…éšœï¼‰
//   - 2xx/3xxï¼šä¸åº”è§¦å‘ failoverï¼ˆæˆåŠŸæˆ–é‡å®šå‘ï¼‰
//
// isQuotaRelated æ ‡è®°ç”¨äºè°ƒåº¦å™¨ä¼˜å…ˆçº§è°ƒæ•´ï¼š
//   - true: é¢åº¦/é…é¢ç›¸å…³ï¼Œé™ä½å¯†é’¥ä¼˜å…ˆçº§
//   - false: ä¸´æ—¶é”™è¯¯ï¼Œä¸å½±å“ä¼˜å…ˆçº§
func shouldRetryWithNextKey(statusCode int, bodyBytes []byte) (bool, bool) {
	// ç¬¬ä¸€å±‚ï¼šåŸºäºçŠ¶æ€ç çš„å¿«é€Ÿåˆ†ç±»
	shouldFailover, isQuotaRelated := classifyByStatusCode(statusCode)
	if shouldFailover {
		return true, isQuotaRelated
	}

	// ç¬¬äºŒå±‚ï¼šè§£æå“åº”ä½“ï¼Œæ£€æŸ¥é”™è¯¯æ¶ˆæ¯
	// ç”¨äº 400/408 ç­‰éœ€è¦è¿›ä¸€æ­¥åˆ¤æ–­çš„çŠ¶æ€ç 
	msgFailover, msgQuota := classifyByErrorMessage(bodyBytes)
	if msgFailover {
		return true, msgQuota
	}

	return false, false
}

// classifyByStatusCode åŸºäº HTTP çŠ¶æ€ç åˆ†ç±»
// è¿”å›: (shouldFailover bool, isQuotaRelated bool)
func classifyByStatusCode(statusCode int) (bool, bool) {
	switch {
	// === è®¤è¯/æˆæƒé”™è¯¯ (åº” failoverï¼Œéé…é¢ç›¸å…³) ===
	case statusCode == 401: // Unauthorized - å¯†é’¥æ— æ•ˆ
		return true, false
	case statusCode == 403: // Forbidden - æƒé™ä¸è¶³
		return true, false

	// === é…é¢/è®¡è´¹é”™è¯¯ (åº” failoverï¼Œé…é¢ç›¸å…³) ===
	case statusCode == 402: // Payment Required - ä½™é¢ä¸è¶³ã€è®¢é˜…è¿‡æœŸ
		return true, true
	case statusCode == 429: // Too Many Requests - é€Ÿç‡é™åˆ¶ã€é…é¢è€—å°½
		return true, true

	// === è¶…æ—¶é”™è¯¯ (åº” failoverï¼Œéé…é¢ç›¸å…³) ===
	case statusCode == 408: // Request Timeout - ä¸Šæ¸¸è¶…æ—¶ï¼Œåº”å°è¯•å…¶ä»–å¯†é’¥/æ¸ é“
		return true, false

	// === éœ€è¦æ£€æŸ¥æ¶ˆæ¯ä½“çš„çŠ¶æ€ç  (äº¤ç»™ç¬¬äºŒå±‚åˆ¤æ–­) ===
	case statusCode == 400: // Bad Request - å¯èƒ½æ˜¯å¯†é’¥æ— æ•ˆã€é…é¢é—®é¢˜ç­‰ï¼Œéœ€æ£€æŸ¥æ¶ˆæ¯ä½“
		return false, false

	// === è¯·æ±‚é”™è¯¯ (ä¸åº” failoverï¼Œå®¢æˆ·ç«¯é—®é¢˜) ===
	case statusCode == 404: // Not Found - ç«¯ç‚¹ä¸å­˜åœ¨ï¼Œæ¢å¯†é’¥æ— æ„ä¹‰
		return false, false
	case statusCode == 405: // Method Not Allowed
		return false, false
	case statusCode == 406: // Not Acceptable
		return false, false
	case statusCode == 409: // Conflict
		return false, false
	case statusCode == 410: // Gone
		return false, false
	case statusCode == 411: // Length Required
		return false, false
	case statusCode == 412: // Precondition Failed
		return false, false
	case statusCode == 413: // Payload Too Large
		return false, false
	case statusCode == 414: // URI Too Long
		return false, false
	case statusCode == 415: // Unsupported Media Type
		return false, false
	case statusCode == 416: // Range Not Satisfiable
		return false, false
	case statusCode == 417: // Expectation Failed
		return false, false
	case statusCode == 422: // Unprocessable Entity - è¯·æ±‚æ ¼å¼æ­£ç¡®ä½†è¯­ä¹‰é”™è¯¯
		return false, false
	case statusCode == 423: // Locked
		return false, false
	case statusCode == 424: // Failed Dependency
		return false, false
	case statusCode == 426: // Upgrade Required
		return false, false
	case statusCode == 428: // Precondition Required
		return false, false
	case statusCode == 431: // Request Header Fields Too Large
		return false, false
	case statusCode == 451: // Unavailable For Legal Reasons
		return false, false

	// === æœåŠ¡ç«¯é”™è¯¯ (åº” failoverï¼Œéé…é¢ç›¸å…³) ===
	case statusCode >= 500: // 5xx æœåŠ¡ç«¯é”™è¯¯
		return true, false

	// === å…¶ä»– 4xx (ä¿å®ˆå¤„ç†ï¼Œä¸ failover) ===
	case statusCode >= 400 && statusCode < 500:
		return false, false

	// === æˆåŠŸ/é‡å®šå‘ (ä¸åº” failover) ===
	default:
		return false, false
	}
}

// classifyByErrorMessage åŸºäºé”™è¯¯æ¶ˆæ¯å†…å®¹åˆ†ç±»
// ç”¨äºå¤„ç†çŠ¶æ€ç æ— æ³•æ˜ç¡®åˆ¤æ–­çš„æƒ…å†µï¼ˆå¦‚ 400/408 é”™è¯¯ï¼‰
// è¿”å›: (shouldFailover bool, isQuotaRelated bool)
func classifyByErrorMessage(bodyBytes []byte) (bool, bool) {
	var errResp map[string]interface{}
	if err := json.Unmarshal(bodyBytes, &errResp); err != nil {
		return false, false
	}

	errObj, ok := errResp["error"].(map[string]interface{})
	if !ok {
		return false, false
	}

	// æ£€æŸ¥ error.message å­—æ®µ
	if msg, ok := errObj["message"].(string); ok {
		if failover, quota := classifyMessage(msg); failover {
			return true, quota
		}
	}

	// æ£€æŸ¥ error.type å­—æ®µ
	if errType, ok := errObj["type"].(string); ok {
		if failover, quota := classifyErrorType(errType); failover {
			return true, quota
		}
	}

	return false, false
}

// classifyMessage åŸºäºé”™è¯¯æ¶ˆæ¯å†…å®¹åˆ†ç±»
// è¿”å›: (shouldFailover bool, isQuotaRelated bool)
func classifyMessage(msg string) (bool, bool) {
	msgLower := strings.ToLower(msg)

	// é…é¢/ä½™é¢ç›¸å…³å…³é”®è¯ (failover + quota)
	quotaKeywords := []string{
		"insufficient", "quota", "credit", "balance",
		"rate limit", "limit exceeded", "exceeded",
		"billing", "payment", "subscription",
		"ç§¯åˆ†ä¸è¶³", "ä½™é¢ä¸è¶³", "è¯·æ±‚æ•°é™åˆ¶", "é¢åº¦",
	}
	for _, keyword := range quotaKeywords {
		if strings.Contains(msgLower, keyword) {
			return true, true
		}
	}

	// è®¤è¯/æˆæƒç›¸å…³å…³é”®è¯ (failover + é quota)
	authKeywords := []string{
		"invalid", "unauthorized", "authentication",
		"api key", "apikey", "token", "expired",
		"permission", "forbidden", "denied",
		"å¯†é’¥æ— æ•ˆ", "è®¤è¯å¤±è´¥", "æƒé™ä¸è¶³",
	}
	for _, keyword := range authKeywords {
		if strings.Contains(msgLower, keyword) {
			return true, false
		}
	}

	// ä¸´æ—¶é”™è¯¯å…³é”®è¯ (failover + é quota)
	transientKeywords := []string{
		"timeout", "timed out", "temporarily",
		"overloaded", "unavailable", "retry",
		"server error", "internal error",
		"è¶…æ—¶", "æš‚æ—¶", "é‡è¯•",
	}
	for _, keyword := range transientKeywords {
		if strings.Contains(msgLower, keyword) {
			return true, false
		}
	}

	return false, false
}

// classifyErrorType åŸºäºé”™è¯¯ç±»å‹åˆ†ç±»
// è¿”å›: (shouldFailover bool, isQuotaRelated bool)
func classifyErrorType(errType string) (bool, bool) {
	typeLower := strings.ToLower(errType)

	// é…é¢ç›¸å…³çš„é”™è¯¯ç±»å‹ (failover + quota)
	quotaTypes := []string{
		"over_quota", "quota_exceeded", "rate_limit",
		"billing", "insufficient", "payment",
	}
	for _, t := range quotaTypes {
		if strings.Contains(typeLower, t) {
			return true, true
		}
	}

	// è®¤è¯ç›¸å…³çš„é”™è¯¯ç±»å‹ (failover + é quota)
	authTypes := []string{
		"authentication", "authorization", "permission",
		"invalid_api_key", "invalid_token", "expired",
	}
	for _, t := range authTypes {
		if strings.Contains(typeLower, t) {
			return true, false
		}
	}

	// æœåŠ¡ç«¯é”™è¯¯ç±»å‹ (failover + é quota)
	serverTypes := []string{
		"server_error", "internal_error", "service_unavailable",
		"timeout", "overloaded",
	}
	for _, t := range serverTypes {
		if strings.Contains(typeLower, t) {
			return true, false
		}
	}

	return false, false
}

// logUsageDetection ç»Ÿä¸€æ ¼å¼è¾“å‡º usage æ£€æµ‹æ—¥å¿—
func logUsageDetection(location string, usage map[string]interface{}, needPatch bool) {
	inputTokens := usage["input_tokens"]
	outputTokens := usage["output_tokens"]
	cacheCreation, _ := usage["cache_creation_input_tokens"].(float64)
	cacheRead, _ := usage["cache_read_input_tokens"].(float64)

	log.Printf("ğŸ”¢ [Stream-Tokenæ£€æµ‹] %s: InputTokens=%v, OutputTokens=%v, CacheCreation=%.0f, CacheRead=%.0f, éœ€è¡¥å…¨=%v",
		location, inputTokens, outputTokens, cacheCreation, cacheRead, needPatch)
}

// buildUsageEvent æ„å»ºå¸¦ usage çš„ message_delta SSE äº‹ä»¶
func buildUsageEvent(requestBody []byte, outputText string) string {
	inputTokens := utils.EstimateRequestTokens(requestBody)
	outputTokens := utils.EstimateTokens(outputText)

	event := map[string]interface{}{
		"type": "message_delta",
		"usage": map[string]int{
			"input_tokens":  inputTokens,
			"output_tokens": outputTokens,
		},
	}
	eventJSON, _ := json.Marshal(event)
	return fmt.Sprintf("event: message_delta\ndata: %s\n\n", eventJSON)
}

// collectedUsageData ä»æµäº‹ä»¶ä¸­æ”¶é›†çš„ usage æ•°æ®
type collectedUsageData struct {
	InputTokens              int
	OutputTokens             int
	CacheCreationInputTokens int
	CacheReadInputTokens     int
}

// extractUsageFromMap ä» usage map ä¸­æå– token æ•°æ®
func extractUsageFromMap(usage map[string]interface{}) collectedUsageData {
	var data collectedUsageData
	if v, ok := usage["input_tokens"].(float64); ok {
		data.InputTokens = int(v)
	}
	if v, ok := usage["output_tokens"].(float64); ok {
		data.OutputTokens = int(v)
	}
	if v, ok := usage["cache_creation_input_tokens"].(float64); ok {
		data.CacheCreationInputTokens = int(v)
	}
	if v, ok := usage["cache_read_input_tokens"].(float64); ok {
		data.CacheReadInputTokens = int(v)
	}
	return data
}

// checkEventUsageStatus æ£€æµ‹äº‹ä»¶æ˜¯å¦åŒ…å« usage å­—æ®µï¼Œå¹¶åˆ¤æ–­æ˜¯å¦éœ€è¦ä¿®è¡¥ input_tokens/output_tokens
// è¿”å›: (hasUsage bool, needPatch bool, usageData collectedUsageData)
func checkEventUsageStatus(event string, enableLog bool) (bool, bool, collectedUsageData) {
	for _, line := range strings.Split(event, "\n") {
		if !strings.HasPrefix(line, "data: ") {
			continue
		}
		jsonStr := strings.TrimPrefix(line, "data: ")

		var data map[string]interface{}
		if err := json.Unmarshal([]byte(jsonStr), &data); err != nil {
			continue
		}

		// æ£€æŸ¥é¡¶å±‚ usage å­—æ®µï¼ˆé€šå¸¸åœ¨ message_delta äº‹ä»¶ï¼‰
		if hasUsage, needInputPatch, needOutputPatch := checkUsageFieldsWithPatch(data["usage"]); hasUsage {
			needPatch := needInputPatch || needOutputPatch
			var usageData collectedUsageData
			if usage, ok := data["usage"].(map[string]interface{}); ok {
				if enableLog {
					logUsageDetection("é¡¶å±‚usage", usage, needPatch)
				}
				usageData = extractUsageFromMap(usage)
			}
			return true, needPatch, usageData
		}

		// æ£€æŸ¥ message.usageï¼ˆClaude message_start äº‹ä»¶æ ¼å¼ï¼‰
		if msg, ok := data["message"].(map[string]interface{}); ok {
			if hasUsage, needInputPatch, needOutputPatch := checkUsageFieldsWithPatch(msg["usage"]); hasUsage {
				needPatch := needInputPatch || needOutputPatch
				var usageData collectedUsageData
				if usage, ok := msg["usage"].(map[string]interface{}); ok {
					if enableLog {
						logUsageDetection("message.usage", usage, needPatch)
					}
					usageData = extractUsageFromMap(usage)
				}
				return true, needPatch, usageData
			}
		}
	}
	return false, false, collectedUsageData{}
}

// checkUsageFieldsWithPatch æ£€æŸ¥ usage å¯¹è±¡æ˜¯å¦åŒ…å« token å­—æ®µï¼Œå¹¶åˆ¤æ–­æ˜¯å¦éœ€è¦ä¿®è¡¥
// è¿”å›: (hasUsage bool, needInputTokenPatch bool, needOutputTokenPatch bool)
// æ³¨æ„ï¼šå¦‚æœæœ‰ cache_creation_input_tokens æˆ– cache_read_input_tokensï¼Œåˆ™ input_tokens ä¸º 0/1 æ˜¯æ­£å¸¸çš„
func checkUsageFieldsWithPatch(usage interface{}) (bool, bool, bool) {
	if u, ok := usage.(map[string]interface{}); ok {
		inputTokens, hasInput := u["input_tokens"]
		outputTokens, hasOutput := u["output_tokens"]
		if hasInput || hasOutput {
			needInputPatch := false
			needOutputPatch := false

			// æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜ tokenï¼ˆå¦‚æœæœ‰ï¼Œinput_tokens ä¸º 0/1 æ˜¯æ­£å¸¸çš„ï¼‰
			cacheCreation, _ := u["cache_creation_input_tokens"].(float64)
			cacheRead, _ := u["cache_read_input_tokens"].(float64)
			hasCacheTokens := cacheCreation > 0 || cacheRead > 0

			if hasInput {
				// åªæœ‰åœ¨æ²¡æœ‰ç¼“å­˜ token çš„æƒ…å†µä¸‹æ‰æ ‡è®°éœ€è¦è¡¥å…¨ input_tokens
				if v, ok := inputTokens.(float64); ok && v <= 1 && !hasCacheTokens {
					needInputPatch = true
				}
			}
			if hasOutput {
				if v, ok := outputTokens.(float64); ok && v <= 1 {
					needOutputPatch = true
				}
			}
			return true, needInputPatch, needOutputPatch
		}
	}
	return false, false, false
}

// hasEventWithUsage æ£€æŸ¥äº‹ä»¶æ˜¯å¦åŒ…å« usage å­—æ®µï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦ä¿®è¡¥ï¼‰
func hasEventWithUsage(event string) bool {
	for _, line := range strings.Split(event, "\n") {
		if !strings.HasPrefix(line, "data: ") {
			continue
		}
		jsonStr := strings.TrimPrefix(line, "data: ")

		var data map[string]interface{}
		if err := json.Unmarshal([]byte(jsonStr), &data); err != nil {
			continue
		}

		// æ£€æŸ¥é¡¶å±‚ usage å­—æ®µ
		if _, ok := data["usage"].(map[string]interface{}); ok {
			return true
		}

		// æ£€æŸ¥ message.usage
		if msg, ok := data["message"].(map[string]interface{}); ok {
			if _, ok := msg["usage"].(map[string]interface{}); ok {
				return true
			}
		}
	}
	return false
}

// patchTokensInEvent ä¿®è¡¥äº‹ä»¶ä¸­çš„ input_tokens å’Œ output_tokens å­—æ®µ
// hasCacheTokens: ä» ctx.collectedUsage ä¼ å…¥ï¼Œåˆ¤æ–­æ˜¯å¦ä¸ºç¼“å­˜è¯·æ±‚ï¼ˆä¸èƒ½ä»å½“å‰äº‹ä»¶è¯»å–ï¼Œå› ä¸ºæœ€ç»ˆäº‹ä»¶é€šå¸¸ä¸å«ç¼“å­˜å­—æ®µï¼‰
func patchTokensInEvent(event string, estimatedInputTokens, estimatedOutputTokens int, hasCacheTokens bool, enableLog bool) string {
	var result strings.Builder
	lines := strings.Split(event, "\n")

	for _, line := range lines {
		if !strings.HasPrefix(line, "data: ") {
			result.WriteString(line)
			result.WriteString("\n")
			continue
		}

		jsonStr := strings.TrimPrefix(line, "data: ")
		var data map[string]interface{}
		if err := json.Unmarshal([]byte(jsonStr), &data); err != nil {
			result.WriteString(line)
			result.WriteString("\n")
			continue
		}

		// ä¿®è¡¥é¡¶å±‚ usage
		if usage, ok := data["usage"].(map[string]interface{}); ok {
			patchUsageFieldsWithLog(usage, estimatedInputTokens, estimatedOutputTokens, hasCacheTokens, enableLog, "é¡¶å±‚usage")
		}

		// ä¿®è¡¥ message.usage
		if msg, ok := data["message"].(map[string]interface{}); ok {
			if usage, ok := msg["usage"].(map[string]interface{}); ok {
				patchUsageFieldsWithLog(usage, estimatedInputTokens, estimatedOutputTokens, hasCacheTokens, enableLog, "message.usage")
			}
		}

		// é‡æ–°åºåˆ—åŒ–
		patchedJSON, err := json.Marshal(data)
		if err != nil {
			result.WriteString(line)
			result.WriteString("\n")
			continue
		}

		result.WriteString("data: ")
		result.Write(patchedJSON)
		result.WriteString("\n")
	}

	return result.String()
}

// patchUsageFieldsWithLog ä¿®è¡¥ usage å¯¹è±¡ä¸­çš„ token å­—æ®µï¼Œå¹¶è¾“å‡ºæ—¥å¿—
// hasCacheTokens: ä» ctx.collectedUsage ä¼ å…¥ï¼ˆè€Œéä»å½“å‰äº‹ä»¶è¯»å–ï¼‰ï¼Œå› ä¸ºæœ€ç»ˆäº‹ä»¶é€šå¸¸ä¸å«ç¼“å­˜å­—æ®µ
// estimatedInput/estimatedOutput: æ”¶é›†åˆ°çš„æœ€å¤§å€¼ï¼ˆæˆ–ä¼°ç®—å€¼ï¼‰
func patchUsageFieldsWithLog(usage map[string]interface{}, estimatedInput, estimatedOutput int, hasCacheTokens bool, enableLog bool, location string) {
	originalInput := usage["input_tokens"]
	originalOutput := usage["output_tokens"]
	inputPatched := false
	outputPatched := false

	// ä»å½“å‰äº‹ä»¶è¯»å–ç¼“å­˜ tokenï¼ˆä»…ç”¨äºæ—¥å¿—è¾“å‡ºï¼Œä¸ç”¨äºåˆ¤æ–­æ˜¯å¦è¡¥å…¨ï¼‰
	cacheCreation, _ := usage["cache_creation_input_tokens"].(float64)
	cacheRead, _ := usage["cache_read_input_tokens"].(float64)
	promptTokens, _ := usage["prompt_tokens"].(float64)
	completionTokens, _ := usage["completion_tokens"].(float64)

	// è¡¥å…¨ input_tokensï¼š
	// 1. å¦‚æœå½“å‰å€¼ <= 1 ä¸”æ²¡æœ‰ç¼“å­˜ tokenï¼Œä½¿ç”¨æ”¶é›†åˆ°çš„å€¼
	// 2. å¦‚æœæ”¶é›†åˆ°çš„å€¼ > å½“å‰å€¼ä¸”æ²¡æœ‰ç¼“å­˜ tokenï¼Œä¹Ÿä½¿ç”¨æ”¶é›†åˆ°çš„å€¼ï¼ˆä¸­é—´äº‹ä»¶å¯èƒ½æœ‰æ›´å‡†ç¡®çš„å€¼ï¼‰
	// æ³¨æ„ï¼šç¼“å­˜è¯·æ±‚åˆæ³•åœ°æŠ¥å‘Š input_tokens ä¸º 0/1ï¼Œä¸åº”è¢«è¦†ç›–
	if v, ok := usage["input_tokens"].(float64); ok {
		currentInput := int(v)
		if !hasCacheTokens && ((currentInput <= 1) || (estimatedInput > currentInput && estimatedInput > 1)) {
			usage["input_tokens"] = estimatedInput
			inputPatched = true
		}
	}
	// è¡¥å…¨ output_tokensï¼š
	// 1. å¦‚æœå½“å‰å€¼ <= 1ï¼Œä½¿ç”¨æ”¶é›†åˆ°çš„å€¼
	// 2. å¦‚æœæ”¶é›†åˆ°çš„å€¼ > å½“å‰å€¼ï¼Œä¹Ÿä½¿ç”¨æ”¶é›†åˆ°çš„å€¼
	if v, ok := usage["output_tokens"].(float64); ok {
		currentOutput := int(v)
		if currentOutput <= 1 || (estimatedOutput > currentOutput && estimatedOutput > 1) {
			usage["output_tokens"] = estimatedOutput
			outputPatched = true
		}
	}

	if enableLog {
		if inputPatched || outputPatched {
			log.Printf("ğŸ”¢ [Stream-Tokenè¡¥å…¨] %s: InputTokens=%vâ†’%v, OutputTokens=%vâ†’%v",
				location, originalInput, usage["input_tokens"], originalOutput, usage["output_tokens"])
		}
		// è®°å½•å®Œæ•´çš„ token ä¿¡æ¯
		log.Printf("ğŸ”¢ [Stream-Tokenç»Ÿè®¡] %s: InputTokens=%v, OutputTokens=%v, CacheCreationInputTokens=%.0f, CacheReadInputTokens=%.0f, PromptTokens=%.0f, CompletionTokens=%.0f",
			location, usage["input_tokens"], usage["output_tokens"], cacheCreation, cacheRead, promptTokens, completionTokens)
	}
}

// isMessageStopEvent ä½¿ç”¨ JSON è§£ææ£€æµ‹æ˜¯å¦ä¸º message_stop äº‹ä»¶
func isMessageStopEvent(event string) bool {
	// å…ˆæ£€æŸ¥ event: è¡Œ
	if strings.Contains(event, "event: message_stop") {
		return true
	}

	// å†æ£€æŸ¥ data ä¸­çš„ type å­—æ®µ
	for _, line := range strings.Split(event, "\n") {
		if !strings.HasPrefix(line, "data: ") {
			continue
		}
		jsonStr := strings.TrimPrefix(line, "data: ")

		var data map[string]interface{}
		if err := json.Unmarshal([]byte(jsonStr), &data); err != nil {
			continue
		}

		if data["type"] == "message_stop" {
			return true
		}
	}
	return false
}

// isMessageDeltaEvent æ£€æµ‹æ˜¯å¦ä¸º message_delta äº‹ä»¶ï¼ˆæµç»“æŸæ—¶åŒ…å«æœ€ç»ˆ usageï¼‰
func isMessageDeltaEvent(event string) bool {
	if strings.Contains(event, "event: message_delta") {
		return true
	}
	for _, line := range strings.Split(event, "\n") {
		if !strings.HasPrefix(line, "data: ") {
			continue
		}
		jsonStr := strings.TrimPrefix(line, "data: ")
		var data map[string]interface{}
		if err := json.Unmarshal([]byte(jsonStr), &data); err != nil {
			continue
		}
		if data["type"] == "message_delta" {
			return true
		}
	}
	return false
}

// extractTextFromEvent ä» SSE äº‹ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹
func extractTextFromEvent(event string, buf *bytes.Buffer) {
	for _, line := range strings.Split(event, "\n") {
		if !strings.HasPrefix(line, "data: ") {
			continue
		}
		jsonStr := strings.TrimPrefix(line, "data: ")

		var data map[string]interface{}
		if err := json.Unmarshal([]byte(jsonStr), &data); err != nil {
			continue
		}

		// Claude SSE: delta.text (text_delta ç±»å‹)
		if delta, ok := data["delta"].(map[string]interface{}); ok {
			if text, ok := delta["text"].(string); ok {
				buf.WriteString(text)
			}
			// tool_use: delta.partial_json
			if partialJSON, ok := delta["partial_json"].(string); ok {
				buf.WriteString(partialJSON)
			}
		}

		// content_block_start ä¸­çš„åˆå§‹æ–‡æœ¬
		if cb, ok := data["content_block"].(map[string]interface{}); ok {
			if text, ok := cb["text"].(string); ok {
				buf.WriteString(text)
			}
		}
	}
}

// CountTokensHandler å¤„ç† /v1/messages/count_tokens è¯·æ±‚
// æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
// 1. ä»£ç†æ¨¡å¼ï¼šè½¬å‘åˆ°ä¸Šæ¸¸è·å–ç²¾ç¡®è®¡æ•°ï¼ˆéœ€è¦ä¸Šæ¸¸æ”¯æŒï¼‰
// 2. æœ¬åœ°ä¼°ç®—æ¨¡å¼ï¼šä½¿ç”¨æœ¬åœ°ç®—æ³•å¿«é€Ÿä¼°ç®—ï¼ˆé»˜è®¤ï¼‰
func CountTokensHandler(envCfg *config.EnvConfig, cfgManager *config.ConfigManager, channelScheduler *scheduler.ChannelScheduler) gin.HandlerFunc {
	return func(c *gin.Context) {
		// è®¤è¯
		middleware.ProxyAuthMiddleware(envCfg)(c)
		if c.IsAborted() {
			return
		}

		// è¯»å–è¯·æ±‚ä½“
		bodyBytes, err := io.ReadAll(c.Request.Body)
		if err != nil {
			c.JSON(400, gin.H{"error": "Failed to read request body"})
			return
		}

		// è§£æè¯·æ±‚
		var req struct {
			Model    string      `json:"model"`
			System   interface{} `json:"system"`
			Messages interface{} `json:"messages"`
			Tools    interface{} `json:"tools"`
		}
		if err := json.Unmarshal(bodyBytes, &req); err != nil {
			c.JSON(400, gin.H{"error": "Invalid JSON"})
			return
		}

		// æœ¬åœ°ä¼°ç®— token æ•°é‡
		inputTokens := utils.EstimateRequestTokens(bodyBytes)

		// è¿”å› Claude API å…¼å®¹çš„å“åº”æ ¼å¼
		c.JSON(200, gin.H{
			"input_tokens": inputTokens,
		})

		if envCfg.EnableResponseLogs {
			log.Printf("ğŸ”¢ [CountTokens] æœ¬åœ°ä¼°ç®—: model=%s, input_tokens=%d", req.Model, inputTokens)
		}
	}
}
