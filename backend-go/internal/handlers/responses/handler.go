// Package responses æä¾› Responses API çš„å¤„ç†å™¨
package responses

import (
	"bufio"
	"bytes"
	"encoding/json"
	"fmt"
	"io"
	"log"
	"net/http"
	"strings"
	"time"

	"github.com/BenedictKing/claude-proxy/internal/config"
	"github.com/BenedictKing/claude-proxy/internal/converters"
	"github.com/BenedictKing/claude-proxy/internal/handlers/common"
	"github.com/BenedictKing/claude-proxy/internal/middleware"
	"github.com/BenedictKing/claude-proxy/internal/providers"
	"github.com/BenedictKing/claude-proxy/internal/scheduler"
	"github.com/BenedictKing/claude-proxy/internal/session"
	"github.com/BenedictKing/claude-proxy/internal/types"
	"github.com/BenedictKing/claude-proxy/internal/utils"
	"github.com/gin-gonic/gin"
)

// Handler Responses API ä»£ç†å¤„ç†å™¨
// æ”¯æŒå¤šæ¸ é“è°ƒåº¦ï¼šå½“é…ç½®å¤šä¸ªæ¸ é“æ—¶è‡ªåŠ¨å¯ç”¨
func Handler(
	envCfg *config.EnvConfig,
	cfgManager *config.ConfigManager,
	sessionManager *session.SessionManager,
	channelScheduler *scheduler.ChannelScheduler,
) gin.HandlerFunc {
	return gin.HandlerFunc(func(c *gin.Context) {
		// å…ˆè¿›è¡Œè®¤è¯
		middleware.ProxyAuthMiddleware(envCfg)(c)
		if c.IsAborted() {
			return
		}

		startTime := time.Now()

		// è¯»å–åŸå§‹è¯·æ±‚ä½“
		maxBodySize := envCfg.MaxRequestBodySize
		bodyBytes, err := common.ReadRequestBody(c, maxBodySize)
		if err != nil {
			return
		}

		// è§£æ Responses è¯·æ±‚
		var responsesReq types.ResponsesRequest
		if len(bodyBytes) > 0 {
			_ = json.Unmarshal(bodyBytes, &responsesReq)
		}

		// æå–å¯¹è¯æ ‡è¯†ç”¨äº Trace äº²å’Œæ€§
		userID := common.ExtractConversationID(c, bodyBytes)

		// æ£€æŸ¥æ˜¯å¦ä¸ºå¤šæ¸ é“æ¨¡å¼
		isMultiChannel := channelScheduler.IsMultiChannelMode(true) // true = isResponses

		if isMultiChannel {
			handleMultiChannel(c, envCfg, cfgManager, channelScheduler, sessionManager, bodyBytes, responsesReq, userID, startTime)
		} else {
			handleSingleChannel(c, envCfg, cfgManager, channelScheduler, sessionManager, bodyBytes, responsesReq, startTime)
		}
	})
}

// handleMultiChannel å¤„ç†å¤šæ¸ é“ Responses è¯·æ±‚
func handleMultiChannel(
	c *gin.Context,
	envCfg *config.EnvConfig,
	cfgManager *config.ConfigManager,
	channelScheduler *scheduler.ChannelScheduler,
	sessionManager *session.SessionManager,
	bodyBytes []byte,
	responsesReq types.ResponsesRequest,
	userID string,
	startTime time.Time,
) {
	failedChannels := make(map[int]bool)
	var lastError error
	var lastFailoverError *common.FailoverError

	maxChannelAttempts := channelScheduler.GetActiveChannelCount(true) // true = isResponses

	for channelAttempt := 0; channelAttempt < maxChannelAttempts; channelAttempt++ {
		selection, err := channelScheduler.SelectChannel(c.Request.Context(), userID, failedChannels, true)
		if err != nil {
			lastError = err
			break
		}

		upstream := selection.Upstream
		channelIndex := selection.ChannelIndex

		if envCfg.ShouldLog("info") {
			log.Printf("ğŸ¯ [å¤šæ¸ é“/Responses] é€‰æ‹©æ¸ é“: [%d] %s (åŸå› : %s, å°è¯• %d/%d)",
				channelIndex, upstream.Name, selection.Reason, channelAttempt+1, maxChannelAttempts)
		}

		success, successKey, failoverErr := tryChannelWithAllKeys(c, envCfg, cfgManager, channelScheduler, sessionManager, upstream, bodyBytes, responsesReq, startTime)

		if success {
			if successKey != "" {
				channelScheduler.RecordSuccess(upstream.BaseURL, successKey, true)
			}
			channelScheduler.SetTraceAffinity(userID, channelIndex)
			return
		}

		failedChannels[channelIndex] = true

		if failoverErr != nil {
			lastFailoverError = failoverErr
			lastError = fmt.Errorf("æ¸ é“ [%d] %s å¤±è´¥", channelIndex, upstream.Name)
		}

		log.Printf("âš ï¸ [å¤šæ¸ é“/Responses] æ¸ é“ [%d] %s æ‰€æœ‰å¯†é’¥éƒ½å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæ¸ é“", channelIndex, upstream.Name)
	}

	log.Printf("ğŸ’¥ [å¤šæ¸ é“/Responses] æ‰€æœ‰æ¸ é“éƒ½å¤±è´¥äº†")
	common.HandleAllChannelsFailed(c, cfgManager.GetFuzzyModeEnabled(), lastFailoverError, lastError, "Responses")
}

// tryChannelWithAllKeys å°è¯•ä½¿ç”¨ Responses æ¸ é“çš„æ‰€æœ‰å¯†é’¥
func tryChannelWithAllKeys(
	c *gin.Context,
	envCfg *config.EnvConfig,
	cfgManager *config.ConfigManager,
	channelScheduler *scheduler.ChannelScheduler,
	sessionManager *session.SessionManager,
	upstream *config.UpstreamConfig,
	bodyBytes []byte,
	responsesReq types.ResponsesRequest,
	startTime time.Time,
) (bool, string, *common.FailoverError) {
	if len(upstream.APIKeys) == 0 {
		return false, "", nil
	}

	provider := &providers.ResponsesProvider{SessionManager: sessionManager}
	metricsManager := channelScheduler.GetResponsesMetricsManager()

	maxRetries := len(upstream.APIKeys)
	failedKeys := make(map[string]bool)
	var lastFailoverError *common.FailoverError
	deprioritizeCandidates := make(map[string]bool)

	// å¼ºåˆ¶æ¢æµ‹æ¨¡å¼
	forceProbeMode := common.AreAllKeysSuspended(metricsManager, upstream.BaseURL, upstream.APIKeys)
	if forceProbeMode {
		log.Printf("ğŸ” [å¼ºåˆ¶æ¢æµ‹/Responses] æ¸ é“ %s æ‰€æœ‰ Key éƒ½è¢«ç†”æ–­ï¼Œå¯ç”¨å¼ºåˆ¶æ¢æµ‹æ¨¡å¼", upstream.Name)
	}

	for attempt := 0; attempt < maxRetries; attempt++ {
		common.RestoreRequestBody(c, bodyBytes)

		apiKey, err := cfgManager.GetNextResponsesAPIKey(upstream, failedKeys)
		if err != nil {
			break
		}

		// æ£€æŸ¥ç†”æ–­çŠ¶æ€
		if !forceProbeMode && metricsManager.ShouldSuspendKey(upstream.BaseURL, apiKey) {
			failedKeys[apiKey] = true
			log.Printf("âš¡ [Responses] è·³è¿‡ç†”æ–­ä¸­çš„ Key: %s", utils.MaskAPIKey(apiKey))
			continue
		}

		if envCfg.ShouldLog("info") {
			log.Printf("ğŸ”‘ [Responses] ä½¿ç”¨APIå¯†é’¥: %s (å°è¯• %d/%d)", utils.MaskAPIKey(apiKey), attempt+1, maxRetries)
		}

		providerReq, _, err := provider.ConvertToProviderRequest(c, upstream, apiKey)
		if err != nil {
			failedKeys[apiKey] = true
			channelScheduler.RecordFailure(upstream.BaseURL, apiKey, true)
			continue
		}

		resp, err := common.SendRequest(providerReq, upstream, envCfg, responsesReq.Stream)
		if err != nil {
			failedKeys[apiKey] = true
			cfgManager.MarkKeyAsFailed(apiKey)
			channelScheduler.RecordFailure(upstream.BaseURL, apiKey, true)
			log.Printf("âš ï¸ [Responses] APIå¯†é’¥å¤±è´¥: %v", err)
			continue
		}

		if resp.StatusCode < 200 || resp.StatusCode >= 300 {
			respBodyBytes, _ := io.ReadAll(resp.Body)
			resp.Body.Close()
			respBodyBytes = utils.DecompressGzipIfNeeded(resp, respBodyBytes)

			shouldFailover, isQuotaRelated := common.ShouldRetryWithNextKey(resp.StatusCode, respBodyBytes, cfgManager.GetFuzzyModeEnabled())
			if shouldFailover {
				failedKeys[apiKey] = true
				cfgManager.MarkKeyAsFailed(apiKey)
				channelScheduler.RecordFailure(upstream.BaseURL, apiKey, true)
				log.Printf("âš ï¸ [Responses] APIå¯†é’¥å¤±è´¥ (çŠ¶æ€: %d)ï¼Œå°è¯•ä¸‹ä¸€ä¸ªå¯†é’¥", resp.StatusCode)

				lastFailoverError = &common.FailoverError{
					Status: resp.StatusCode,
					Body:   respBodyBytes,
				}

				if isQuotaRelated {
					deprioritizeCandidates[apiKey] = true
				}
				continue
			}

			// é failover é”™è¯¯ï¼Œè®°å½•å¤±è´¥æŒ‡æ ‡åè¿”å›
			channelScheduler.RecordFailure(upstream.BaseURL, apiKey, true)
			c.Data(resp.StatusCode, "application/json", respBodyBytes)
			return true, "", nil
		}

		if len(deprioritizeCandidates) > 0 {
			for key := range deprioritizeCandidates {
				_ = cfgManager.DeprioritizeAPIKey(key)
			}
		}

		handleSuccess(c, resp, provider, upstream.ServiceType, envCfg, sessionManager, startTime, &responsesReq, bodyBytes)
		return true, apiKey, nil
	}

	return false, "", lastFailoverError
}

// handleSingleChannel å¤„ç†å•æ¸ é“ Responses è¯·æ±‚
func handleSingleChannel(
	c *gin.Context,
	envCfg *config.EnvConfig,
	cfgManager *config.ConfigManager,
	channelScheduler *scheduler.ChannelScheduler,
	sessionManager *session.SessionManager,
	bodyBytes []byte,
	responsesReq types.ResponsesRequest,
	startTime time.Time,
) {
	upstream, err := cfgManager.GetCurrentResponsesUpstream()
	if err != nil {
		c.JSON(503, gin.H{
			"error": "æœªé…ç½®ä»»ä½• Responses æ¸ é“ï¼Œè¯·å…ˆåœ¨ç®¡ç†ç•Œé¢æ·»åŠ æ¸ é“",
			"code":  "NO_RESPONSES_UPSTREAM",
		})
		return
	}

	if len(upstream.APIKeys) == 0 {
		c.JSON(503, gin.H{
			"error": fmt.Sprintf("å½“å‰ Responses æ¸ é“ \"%s\" æœªé…ç½®APIå¯†é’¥", upstream.Name),
			"code":  "NO_API_KEYS",
		})
		return
	}

	provider := &providers.ResponsesProvider{SessionManager: sessionManager}

	maxRetries := len(upstream.APIKeys)
	failedKeys := make(map[string]bool)
	var lastError error
	var lastOriginalBodyBytes []byte
	var lastFailoverError *common.FailoverError
	deprioritizeCandidates := make(map[string]bool)

	for attempt := 0; attempt < maxRetries; attempt++ {
		common.RestoreRequestBody(c, bodyBytes)

		apiKey, err := cfgManager.GetNextResponsesAPIKey(upstream, failedKeys)
		if err != nil {
			lastError = err
			break
		}

		if envCfg.ShouldLog("info") {
			log.Printf("ğŸ¯ ä½¿ç”¨ Responses ä¸Šæ¸¸: %s - %s (å°è¯• %d/%d)", upstream.Name, upstream.BaseURL, attempt+1, maxRetries)
			log.Printf("ğŸ”‘ ä½¿ç”¨APIå¯†é’¥: %s", utils.MaskAPIKey(apiKey))
		}

		providerReq, originalBodyBytes, err := provider.ConvertToProviderRequest(c, upstream, apiKey)
		if err != nil {
			lastError = err
			failedKeys[apiKey] = true
			if originalBodyBytes != nil {
				lastOriginalBodyBytes = originalBodyBytes
			}
			continue
		}
		lastOriginalBodyBytes = originalBodyBytes

		common.LogOriginalRequest(c, lastOriginalBodyBytes, envCfg, "Responses")

		resp, err := common.SendRequest(providerReq, upstream, envCfg, responsesReq.Stream)
		if err != nil {
			lastError = err
			failedKeys[apiKey] = true
			cfgManager.MarkKeyAsFailed(apiKey)
			log.Printf("âš ï¸ APIå¯†é’¥å¤±è´¥: %v", err)
			continue
		}

		if resp.StatusCode < 200 || resp.StatusCode >= 300 {
			respBodyBytes, _ := io.ReadAll(resp.Body)
			resp.Body.Close()
			respBodyBytes = utils.DecompressGzipIfNeeded(resp, respBodyBytes)

			shouldFailover, isQuotaRelated := common.ShouldRetryWithNextKey(resp.StatusCode, respBodyBytes, cfgManager.GetFuzzyModeEnabled())
			if shouldFailover {
				lastError = fmt.Errorf("ä¸Šæ¸¸é”™è¯¯: %d", resp.StatusCode)
				failedKeys[apiKey] = true
				cfgManager.MarkKeyAsFailed(apiKey)

				log.Printf("âš ï¸ Responses APIå¯†é’¥å¤±è´¥ (çŠ¶æ€: %d)ï¼Œå°è¯•ä¸‹ä¸€ä¸ªå¯†é’¥", resp.StatusCode)
				if envCfg.EnableResponseLogs && envCfg.IsDevelopment() {
					var formattedBody string
					if envCfg.RawLogOutput {
						formattedBody = utils.FormatJSONBytesRaw(respBodyBytes)
					} else {
						formattedBody = utils.FormatJSONBytesForLog(respBodyBytes, 500)
					}
					log.Printf("ğŸ“¦ å¤±è´¥åŸå› :\n%s", formattedBody)
				} else if envCfg.EnableResponseLogs {
					log.Printf("å¤±è´¥åŸå› : %s", string(respBodyBytes))
				}

				lastFailoverError = &common.FailoverError{
					Status: resp.StatusCode,
					Body:   respBodyBytes,
				}

				if isQuotaRelated {
					deprioritizeCandidates[apiKey] = true
				}
				continue
			}

			// é failover é”™è¯¯ï¼Œè®°å½•å¤±è´¥æŒ‡æ ‡åè¿”å›
			if envCfg.EnableResponseLogs {
				log.Printf("âš ï¸ Responses ä¸Šæ¸¸è¿”å›é”™è¯¯: %d", resp.StatusCode)
				if envCfg.IsDevelopment() {
					var formattedBody string
					if envCfg.RawLogOutput {
						formattedBody = utils.FormatJSONBytesRaw(respBodyBytes)
					} else {
						formattedBody = utils.FormatJSONBytesForLog(respBodyBytes, 500)
					}
					log.Printf("ğŸ“¦ é”™è¯¯å“åº”ä½“:\n%s", formattedBody)

					respHeaders := make(map[string]string)
					for key, values := range resp.Header {
						if len(values) > 0 {
							respHeaders[key] = values[0]
						}
					}
					var respHeadersJSON []byte
					if envCfg.RawLogOutput {
						respHeadersJSON, _ = json.Marshal(respHeaders)
					} else {
						respHeadersJSON, _ = json.MarshalIndent(respHeaders, "", "  ")
					}
					log.Printf("ğŸ“‹ é”™è¯¯å“åº”å¤´:\n%s", string(respHeadersJSON))
				}
			}
			channelScheduler.RecordFailure(upstream.BaseURL, apiKey, true)
			c.Data(resp.StatusCode, "application/json", respBodyBytes)
			return
		}

		if len(deprioritizeCandidates) > 0 {
			for key := range deprioritizeCandidates {
				if err := cfgManager.DeprioritizeAPIKey(key); err != nil {
					log.Printf("âš ï¸ å¯†é’¥é™çº§å¤±è´¥: %v", err)
				}
			}
		}

		handleSuccess(c, resp, provider, upstream.ServiceType, envCfg, sessionManager, startTime, &responsesReq, bodyBytes)
		return
	}

	log.Printf("ğŸ’¥ æ‰€æœ‰ Responses APIå¯†é’¥éƒ½å¤±è´¥äº†")
	common.HandleAllKeysFailed(c, cfgManager.GetFuzzyModeEnabled(), lastFailoverError, lastError, "Responses")
}

// handleSuccess å¤„ç†æˆåŠŸçš„ Responses å“åº”
func handleSuccess(
	c *gin.Context,
	resp *http.Response,
	provider *providers.ResponsesProvider,
	upstreamType string,
	envCfg *config.EnvConfig,
	sessionManager *session.SessionManager,
	startTime time.Time,
	originalReq *types.ResponsesRequest,
	originalRequestJSON []byte,
) {
	defer resp.Body.Close()

	isStream := originalReq != nil && originalReq.Stream

	if isStream {
		handleStreamSuccess(c, resp, upstreamType, envCfg, startTime, originalReq, originalRequestJSON)
		return
	}

	// éæµå¼å“åº”å¤„ç†
	bodyBytes, err := io.ReadAll(resp.Body)
	if err != nil {
		c.JSON(500, gin.H{"error": "Failed to read response"})
		return
	}

	if envCfg.EnableResponseLogs {
		responseTime := time.Since(startTime).Milliseconds()
		log.Printf("â±ï¸ Responses å“åº”å®Œæˆ: %dms, çŠ¶æ€: %d", responseTime, resp.StatusCode)
		if envCfg.IsDevelopment() {
			respHeaders := make(map[string]string)
			for key, values := range resp.Header {
				if len(values) > 0 {
					respHeaders[key] = values[0]
				}
			}
			var respHeadersJSON []byte
			if envCfg.RawLogOutput {
				respHeadersJSON, _ = json.Marshal(respHeaders)
			} else {
				respHeadersJSON, _ = json.MarshalIndent(respHeaders, "", "  ")
			}
			log.Printf("ğŸ“‹ å“åº”å¤´:\n%s", string(respHeadersJSON))

			var formattedBody string
			if envCfg.RawLogOutput {
				formattedBody = utils.FormatJSONBytesRaw(bodyBytes)
			} else {
				formattedBody = utils.FormatJSONBytesForLog(bodyBytes, 500)
			}
			log.Printf("ğŸ“¦ å“åº”ä½“:\n%s", formattedBody)
		}
	}

	providerResp := &types.ProviderResponse{
		StatusCode: resp.StatusCode,
		Headers:    resp.Header,
		Body:       bodyBytes,
		Stream:     false,
	}

	responsesResp, err := provider.ConvertToResponsesResponse(providerResp, upstreamType, "")
	if err != nil {
		c.JSON(500, gin.H{"error": "Failed to convert response"})
		return
	}

	// Token è¡¥å…¨é€»è¾‘
	patchResponsesUsage(responsesResp, originalRequestJSON, envCfg)

	// æ›´æ–°ä¼šè¯
	if originalReq.Store == nil || *originalReq.Store {
		sess, err := sessionManager.GetOrCreateSession(originalReq.PreviousResponseID)
		if err == nil {
			inputItems, _ := parseInputToItems(originalReq.Input)
			for _, item := range inputItems {
				sessionManager.AppendMessage(sess.ID, item, 0)
			}

			for _, item := range responsesResp.Output {
				sessionManager.AppendMessage(sess.ID, item, responsesResp.Usage.TotalTokens)
			}

			sessionManager.UpdateLastResponseID(sess.ID, responsesResp.ID)
			sessionManager.RecordResponseMapping(responsesResp.ID, sess.ID)

			if sess.LastResponseID != "" {
				responsesResp.PreviousID = sess.LastResponseID
			}
		}
	}

	utils.ForwardResponseHeaders(resp.Header, c.Writer)
	c.JSON(200, responsesResp)
}

// patchResponsesUsage è¡¥å…¨ Responses å“åº”çš„ Token ç»Ÿè®¡
func patchResponsesUsage(resp *types.ResponsesResponse, requestBody []byte, envCfg *config.EnvConfig) {
	// æ£€æŸ¥æ˜¯å¦æœ‰ Claude åŸç”Ÿç¼“å­˜ tokenï¼ˆæœ‰æ—¶æ‰è·³è¿‡ input_tokens ä¿®è¡¥ï¼‰
	// ä»…æ£€æµ‹ Claude åŸç”Ÿå­—æ®µï¼šcache_creation_input_tokens, cache_read_input_tokens,
	// cache_creation_5m_input_tokens, cache_creation_1h_input_tokens
	// æ³¨æ„ï¼šä¸æ£€æµ‹ input_tokens_details.cached_tokensï¼ˆOpenAI æ ¼å¼ï¼‰ï¼Œé¿å…é”™è¯¯è·³è¿‡
	hasClaudeCache := resp.Usage.CacheCreationInputTokens > 0 ||
		resp.Usage.CacheReadInputTokens > 0 ||
		resp.Usage.CacheCreation5mInputTokens > 0 ||
		resp.Usage.CacheCreation1hInputTokens > 0

	// æ£€æŸ¥æ˜¯å¦éœ€è¦è¡¥å…¨
	needInputPatch := resp.Usage.InputTokens <= 1 && !hasClaudeCache
	needOutputPatch := resp.Usage.OutputTokens <= 1

	// å¦‚æœ usage å®Œå…¨ä¸ºç©ºï¼Œè¿›è¡Œå®Œæ•´ä¼°ç®—
	if resp.Usage.InputTokens == 0 && resp.Usage.OutputTokens == 0 && resp.Usage.TotalTokens == 0 {
		estimatedInput := utils.EstimateResponsesRequestTokens(requestBody)
		estimatedOutput := estimateResponsesOutputFromItems(resp.Output)
		resp.Usage.InputTokens = estimatedInput
		resp.Usage.OutputTokens = estimatedOutput
		resp.Usage.TotalTokens = estimatedInput + estimatedOutput
		if envCfg.EnableResponseLogs {
			log.Printf("ğŸ”¢ [Responses-Tokenè¡¥å…¨] ä¸Šæ¸¸æ— Usage, æœ¬åœ°ä¼°ç®—: input=%d, output=%d", estimatedInput, estimatedOutput)
		}
		return
	}

	// ä¿®è¡¥è™šå‡å€¼
	originalInput := resp.Usage.InputTokens
	originalOutput := resp.Usage.OutputTokens
	patched := false

	if needInputPatch {
		resp.Usage.InputTokens = utils.EstimateResponsesRequestTokens(requestBody)
		patched = true
	}
	if needOutputPatch {
		resp.Usage.OutputTokens = estimateResponsesOutputFromItems(resp.Output)
		patched = true
	}

	// é‡æ–°è®¡ç®— TotalTokensï¼ˆä¿®è¡¥æ—¶æˆ– total_tokens ä¸º 0 ä½† input/output æœ‰æ•ˆæ—¶ï¼‰
	if patched || (resp.Usage.TotalTokens == 0 && (resp.Usage.InputTokens > 0 || resp.Usage.OutputTokens > 0)) {
		resp.Usage.TotalTokens = resp.Usage.InputTokens + resp.Usage.OutputTokens
	}

	if envCfg.EnableResponseLogs {
		if patched {
			log.Printf("ğŸ”¢ [Responses-Tokenè¡¥å…¨] è™šå‡å€¼: InputTokens=%dâ†’%d, OutputTokens=%dâ†’%d",
				originalInput, resp.Usage.InputTokens, originalOutput, resp.Usage.OutputTokens)
		}
		log.Printf("ğŸ”¢ [Responses-Tokenç»Ÿè®¡] InputTokens=%d, OutputTokens=%d, TotalTokens=%d, CacheCreation=%d, CacheRead=%d, CacheCreation5m=%d, CacheCreation1h=%d, CacheTTL=%s",
			resp.Usage.InputTokens, resp.Usage.OutputTokens, resp.Usage.TotalTokens,
			resp.Usage.CacheCreationInputTokens, resp.Usage.CacheReadInputTokens,
			resp.Usage.CacheCreation5mInputTokens, resp.Usage.CacheCreation1hInputTokens,
			resp.Usage.CacheTTL)
	}
}

// estimateResponsesOutputFromItems ä» ResponsesItem æ•°ç»„ä¼°ç®—è¾“å‡º token
func estimateResponsesOutputFromItems(output []types.ResponsesItem) int {
	if len(output) == 0 {
		return 0
	}

	total := 0
	for _, item := range output {
		// å¤„ç† content
		if item.Content != nil {
			switch v := item.Content.(type) {
			case string:
				total += utils.EstimateTokens(v)
			case []interface{}:
				for _, block := range v {
					if b, ok := block.(map[string]interface{}); ok {
						if text, ok := b["text"].(string); ok {
							total += utils.EstimateTokens(text)
						}
					}
				}
			case []types.ContentBlock:
				// å¤„ç†ç»“æ„åŒ– ContentBlock æ•°ç»„
				for _, block := range v {
					if block.Text != "" {
						total += utils.EstimateTokens(block.Text)
					}
				}
			default:
				// å›é€€ï¼šåºåˆ—åŒ–åä¼°ç®—
				data, _ := json.Marshal(v)
				total += utils.EstimateTokens(string(data))
			}
		}

		// å¤„ç† tool_use
		if item.ToolUse != nil {
			if item.ToolUse.Name != "" {
				total += utils.EstimateTokens(item.ToolUse.Name) + 2
			}
			if item.ToolUse.Input != nil {
				data, _ := json.Marshal(item.ToolUse.Input)
				total += utils.EstimateTokens(string(data))
			}
		}

		// å¤„ç† function_call ç±»å‹ï¼ˆitem.Type == "function_call"ï¼‰
		if item.Type == "function_call" {
			// åœ¨è½¬æ¢åçš„å“åº”ä¸­ï¼Œfunction_call çš„å‚æ•°å¯èƒ½åœ¨ Content ä¸­
			if contentStr, ok := item.Content.(string); ok {
				total += utils.EstimateTokens(contentStr)
			}
		}
	}

	return total
}

// handleStreamSuccess å¤„ç†æµå¼å“åº”
func handleStreamSuccess(
	c *gin.Context,
	resp *http.Response,
	upstreamType string,
	envCfg *config.EnvConfig,
	startTime time.Time,
	originalReq *types.ResponsesRequest,
	originalRequestJSON []byte,
) {
	if envCfg.EnableResponseLogs {
		responseTime := time.Since(startTime).Milliseconds()
		log.Printf("â±ï¸ Responses æµå¼å“åº”å¼€å§‹: %dms, çŠ¶æ€: %d", responseTime, resp.StatusCode)
	}

	utils.ForwardResponseHeaders(resp.Header, c.Writer)

	c.Header("Content-Type", "text/event-stream")
	c.Header("Cache-Control", "no-cache")
	c.Header("Connection", "keep-alive")
	c.Header("X-Accel-Buffering", "no")

	var synthesizer *utils.StreamSynthesizer
	var logBuffer bytes.Buffer
	streamLoggingEnabled := envCfg.IsDevelopment() && envCfg.EnableResponseLogs

	if streamLoggingEnabled {
		synthesizer = utils.NewStreamSynthesizer(upstreamType)
	}

	needConvert := upstreamType != "responses"
	var converterState any

	c.Status(resp.StatusCode)
	flusher, _ := c.Writer.(http.Flusher)

	scanner := bufio.NewScanner(resp.Body)
	const maxCapacity = 1024 * 1024
	buf := make([]byte, 0, 64*1024)
	scanner.Buffer(buf, maxCapacity)

	// Token ç»Ÿè®¡çŠ¶æ€
	var outputTextBuffer bytes.Buffer
	const maxOutputBufferSize = 1024 * 1024 // 1MB ä¸Šé™ï¼Œé˜²æ­¢å†…å­˜æº¢å‡º
	var collectedUsage responsesStreamUsage
	hasUsage := false
	needTokenPatch := false
	clientGone := false

	for scanner.Scan() {
		line := scanner.Text()

		if streamLoggingEnabled {
			logBuffer.WriteString(line + "\n")
			if synthesizer != nil {
				synthesizer.ProcessLine(line)
			}
		}

		// å¤„ç†è½¬æ¢åçš„äº‹ä»¶
		var eventsToProcess []string

		if needConvert {
			events := converters.ConvertOpenAIChatToResponses(
				c.Request.Context(),
				originalReq.Model,
				originalRequestJSON,
				nil,
				[]byte(line),
				&converterState,
			)
			eventsToProcess = events
		} else {
			eventsToProcess = []string{line + "\n"}
		}

		for _, event := range eventsToProcess {
			// æå–æ–‡æœ¬å†…å®¹ç”¨äºä¼°ç®—ï¼ˆé™åˆ¶ç¼“å†²åŒºå¤§å°ï¼‰
			if outputTextBuffer.Len() < maxOutputBufferSize {
				extractResponsesTextFromEvent(event, &outputTextBuffer)
			}

			// æ£€æµ‹å¹¶æ”¶é›† usage
			detected, needPatch, usageData := checkResponsesEventUsage(event, envCfg.EnableResponseLogs && envCfg.ShouldLog("debug"))
			if detected {
				if !hasUsage {
					hasUsage = true
					needTokenPatch = needPatch
					if envCfg.EnableResponseLogs && envCfg.ShouldLog("debug") && needPatch {
						log.Printf("ğŸ”¢ [Responses-Stream-Token] æ£€æµ‹åˆ°è™šå‡å€¼, å»¶è¿Ÿåˆ°æµç»“æŸä¿®è¡¥")
					}
				}
				updateResponsesStreamUsage(&collectedUsage, usageData)
			}

			// åœ¨ response.completed äº‹ä»¶å‰æ³¨å…¥/ä¿®è¡¥ usage
			eventToSend := event
			if isResponsesCompletedEvent(event) {
				if !hasUsage {
					// ä¸Šæ¸¸å®Œå…¨æ²¡æœ‰ usageï¼Œæ³¨å…¥æœ¬åœ°ä¼°ç®—
					eventToSend = injectResponsesUsageToCompletedEvent(event, originalRequestJSON, outputTextBuffer.String(), envCfg)
					if envCfg.EnableResponseLogs && envCfg.ShouldLog("debug") {
						log.Printf("ğŸ”¢ [Responses-Stream-Tokenæ³¨å…¥] ä¸Šæ¸¸æ— usage, æ³¨å…¥æœ¬åœ°ä¼°ç®—")
					}
				} else if needTokenPatch {
					// éœ€è¦ä¿®è¡¥è™šå‡å€¼
					eventToSend = patchResponsesCompletedEventUsage(event, originalRequestJSON, outputTextBuffer.String(), &collectedUsage, envCfg)
				}
			}

			// è½¬å‘ç»™å®¢æˆ·ç«¯
			if !clientGone {
				_, err := c.Writer.Write([]byte(eventToSend))
				if err != nil {
					clientGone = true
					if !isClientDisconnectError(err) {
						log.Printf("âš ï¸ æµå¼å“åº”ä¼ è¾“é”™è¯¯: %v", err)
					} else if envCfg.ShouldLog("info") {
						log.Printf("â„¹ï¸ å®¢æˆ·ç«¯ä¸­æ–­è¿æ¥ (æ­£å¸¸è¡Œä¸º)ï¼Œç»§ç»­æ¥æ”¶ä¸Šæ¸¸æ•°æ®...")
					}
				} else if flusher != nil {
					flusher.Flush()
				}
			}
		}
	}

	if err := scanner.Err(); err != nil {
		log.Printf("âš ï¸ æµå¼å“åº”è¯»å–é”™è¯¯: %v", err)
	}

	if envCfg.EnableResponseLogs {
		responseTime := time.Since(startTime).Milliseconds()
		log.Printf("âœ… Responses æµå¼å“åº”å®Œæˆ: %dms", responseTime)

		// è¾“å‡º Token ç»Ÿè®¡
		if hasUsage || collectedUsage.InputTokens > 0 || collectedUsage.OutputTokens > 0 {
			log.Printf("ğŸ”¢ [Responses-Stream-Tokenç»Ÿè®¡] InputTokens=%d, OutputTokens=%d, CacheCreation=%d, CacheRead=%d, CacheCreation5m=%d, CacheCreation1h=%d, CacheTTL=%s",
				collectedUsage.InputTokens, collectedUsage.OutputTokens,
				collectedUsage.CacheCreationInputTokens, collectedUsage.CacheReadInputTokens,
				collectedUsage.CacheCreation5mInputTokens, collectedUsage.CacheCreation1hInputTokens,
				collectedUsage.CacheTTL)
		}

		if envCfg.IsDevelopment() {
			if synthesizer != nil {
				synthesizedContent := synthesizer.GetSynthesizedContent()
				parseFailed := synthesizer.IsParseFailed()
				if synthesizedContent != "" && !parseFailed {
					log.Printf("ğŸ›°ï¸  ä¸Šæ¸¸æµå¼å“åº”åˆæˆå†…å®¹:\n%s", strings.TrimSpace(synthesizedContent))
				} else if logBuffer.Len() > 0 {
					log.Printf("ğŸ›°ï¸  ä¸Šæ¸¸æµå¼å“åº”åŸå§‹å†…å®¹:\n%s", logBuffer.String())
				}
			} else if logBuffer.Len() > 0 {
				log.Printf("ğŸ›°ï¸  ä¸Šæ¸¸æµå¼å“åº”åŸå§‹å†…å®¹:\n%s", logBuffer.String())
			}
		}
	}
}

// responsesStreamUsage æµå¼å“åº” usage æ”¶é›†ç»“æ„
type responsesStreamUsage struct {
	InputTokens                int
	OutputTokens               int
	TotalTokens                int // ç”¨äºæ£€æµ‹ total_tokens æ˜¯å¦éœ€è¦è¡¥å…¨
	CacheCreationInputTokens   int
	CacheReadInputTokens       int
	CacheCreation5mInputTokens int
	CacheCreation1hInputTokens int
	CacheTTL                   string
	HasClaudeCache             bool // æ˜¯å¦æ£€æµ‹åˆ° Claude åŸç”Ÿç¼“å­˜å­—æ®µï¼ˆåŒºåˆ«äº OpenAI cached_tokensï¼‰
}

// extractResponsesTextFromEvent ä» Responses SSE äº‹ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹
func extractResponsesTextFromEvent(event string, buf *bytes.Buffer) {
	for _, line := range strings.Split(event, "\n") {
		if !strings.HasPrefix(line, "data: ") {
			continue
		}
		jsonStr := strings.TrimPrefix(line, "data: ")

		var data map[string]interface{}
		if err := json.Unmarshal([]byte(jsonStr), &data); err != nil {
			continue
		}

		eventType, _ := data["type"].(string)

		// å¤„ç†å„ç§ delta ç±»å‹
		switch eventType {
		case "response.output_text.delta":
			if delta, ok := data["delta"].(string); ok {
				buf.WriteString(delta)
			}
		case "response.function_call_arguments.delta":
			if delta, ok := data["delta"].(string); ok {
				buf.WriteString(delta)
			}
		case "response.reasoning_summary_text.delta":
			if text, ok := data["text"].(string); ok {
				buf.WriteString(text)
			}
		case "response.output_json.delta":
			// JSON è¾“å‡ºå¢é‡
			if delta, ok := data["delta"].(string); ok {
				buf.WriteString(delta)
			}
		case "response.content_part.delta":
			// å†…å®¹å—å¢é‡ï¼ˆé€šç”¨ï¼‰
			if delta, ok := data["delta"].(string); ok {
				buf.WriteString(delta)
			} else if text, ok := data["text"].(string); ok {
				buf.WriteString(text)
			}
		case "response.audio.delta", "response.audio_transcript.delta":
			// éŸ³é¢‘è½¬å½•å¢é‡
			if delta, ok := data["delta"].(string); ok {
				buf.WriteString(delta)
			}
		}
	}
}

// checkResponsesEventUsage æ£€æµ‹ Responses äº‹ä»¶æ˜¯å¦åŒ…å« usage
func checkResponsesEventUsage(event string, enableLog bool) (bool, bool, responsesStreamUsage) {
	for _, line := range strings.Split(event, "\n") {
		if !strings.HasPrefix(line, "data: ") {
			continue
		}
		jsonStr := strings.TrimPrefix(line, "data: ")

		var data map[string]interface{}
		if err := json.Unmarshal([]byte(jsonStr), &data); err != nil {
			continue
		}

		// æ£€æŸ¥ response.completed äº‹ä»¶ä¸­çš„ usage
		if data["type"] == "response.completed" {
			if response, ok := data["response"].(map[string]interface{}); ok {
				if usage, ok := response["usage"].(map[string]interface{}); ok {
					usageData := extractResponsesUsageFromMap(usage)
					needPatch := usageData.InputTokens <= 1 || usageData.OutputTokens <= 1

					// ä»…å½“æ£€æµ‹åˆ° Claude åŸç”Ÿç¼“å­˜å­—æ®µæ—¶ï¼Œæ‰è·³è¿‡ input_tokens è¡¥å…¨
					// OpenAI çš„ input_tokens_details.cached_tokens ä¸åº”é˜»æ­¢è¡¥å…¨
					if usageData.HasClaudeCache && usageData.InputTokens <= 1 {
						needPatch = usageData.OutputTokens <= 1 // æœ‰ Claude ç¼“å­˜æ—¶åªæ£€æŸ¥ output
					}

					// æ£€æŸ¥ total_tokens æ˜¯å¦éœ€è¦è¡¥å…¨ï¼ˆæœ‰æ•ˆ input/output ä½† total=0ï¼‰
					if !needPatch && usageData.TotalTokens == 0 && (usageData.InputTokens > 0 || usageData.OutputTokens > 0) {
						needPatch = true
					}

					if enableLog {
						log.Printf("ğŸ”¢ [Responses-Stream-Tokenæ£€æµ‹] response.completed: InputTokens=%d, OutputTokens=%d, TotalTokens=%d, HasClaudeCache=%v, éœ€è¡¥å…¨=%v",
							usageData.InputTokens, usageData.OutputTokens, usageData.TotalTokens, usageData.HasClaudeCache, needPatch)
					}
					return true, needPatch, usageData
				}
			}
		}
	}
	return false, false, responsesStreamUsage{}
}

// extractResponsesUsageFromMap ä» usage map ä¸­æå–æ•°æ®
func extractResponsesUsageFromMap(usage map[string]interface{}) responsesStreamUsage {
	var data responsesStreamUsage

	if v, ok := usage["input_tokens"].(float64); ok {
		data.InputTokens = int(v)
	}
	if v, ok := usage["output_tokens"].(float64); ok {
		data.OutputTokens = int(v)
	}
	if v, ok := usage["total_tokens"].(float64); ok {
		data.TotalTokens = int(v)
	}
	if v, ok := usage["cache_creation_input_tokens"].(float64); ok {
		data.CacheCreationInputTokens = int(v)
		if v > 0 {
			data.HasClaudeCache = true
		}
	}
	if v, ok := usage["cache_read_input_tokens"].(float64); ok {
		data.CacheReadInputTokens = int(v)
		if v > 0 {
			data.HasClaudeCache = true
		}
	}
	if v, ok := usage["cache_creation_5m_input_tokens"].(float64); ok {
		data.CacheCreation5mInputTokens = int(v)
		if v > 0 {
			data.HasClaudeCache = true
		}
	}
	if v, ok := usage["cache_creation_1h_input_tokens"].(float64); ok {
		data.CacheCreation1hInputTokens = int(v)
		if v > 0 {
			data.HasClaudeCache = true
		}
	}

	// æ£€æŸ¥ input_tokens_details.cached_tokens (OpenAI æ ¼å¼ï¼Œä¸è®¾ç½® HasClaudeCache)
	if details, ok := usage["input_tokens_details"].(map[string]interface{}); ok {
		if cached, ok := details["cached_tokens"].(float64); ok && cached > 0 {
			// ä»…å½“ CacheReadInputTokens æœªè¢«è®¾ç½®æ—¶æ‰ä½¿ç”¨ OpenAI çš„ cached_tokens
			if data.CacheReadInputTokens == 0 {
				data.CacheReadInputTokens = int(cached)
			}
			// æ³¨æ„ï¼šä¸è®¾ç½® HasClaudeCacheï¼Œå› ä¸ºè¿™æ˜¯ OpenAI æ ¼å¼
		}
	}

	// è®¾ç½® CacheTTL
	var has5m, has1h bool
	if data.CacheCreation5mInputTokens > 0 {
		has5m = true
	}
	if data.CacheCreation1hInputTokens > 0 {
		has1h = true
	}
	if has5m && has1h {
		data.CacheTTL = "mixed"
	} else if has1h {
		data.CacheTTL = "1h"
	} else if has5m {
		data.CacheTTL = "5m"
	}

	return data
}

// updateResponsesStreamUsage æ›´æ–°æ”¶é›†çš„ usage æ•°æ®
func updateResponsesStreamUsage(collected *responsesStreamUsage, usageData responsesStreamUsage) {
	if usageData.InputTokens > collected.InputTokens {
		collected.InputTokens = usageData.InputTokens
	}
	if usageData.OutputTokens > collected.OutputTokens {
		collected.OutputTokens = usageData.OutputTokens
	}
	if usageData.TotalTokens > collected.TotalTokens {
		collected.TotalTokens = usageData.TotalTokens
	}
	if usageData.CacheCreationInputTokens > 0 {
		collected.CacheCreationInputTokens = usageData.CacheCreationInputTokens
	}
	if usageData.CacheReadInputTokens > 0 {
		collected.CacheReadInputTokens = usageData.CacheReadInputTokens
	}
	if usageData.CacheCreation5mInputTokens > 0 {
		collected.CacheCreation5mInputTokens = usageData.CacheCreation5mInputTokens
	}
	if usageData.CacheCreation1hInputTokens > 0 {
		collected.CacheCreation1hInputTokens = usageData.CacheCreation1hInputTokens
	}
	if usageData.CacheTTL != "" {
		collected.CacheTTL = usageData.CacheTTL
	}
	// ä¼ æ’­ HasClaudeCache æ ‡å¿—
	if usageData.HasClaudeCache {
		collected.HasClaudeCache = true
	}
}

// isResponsesCompletedEvent æ£€æµ‹æ˜¯å¦ä¸º response.completed äº‹ä»¶
func isResponsesCompletedEvent(event string) bool {
	return strings.Contains(event, `"type":"response.completed"`) ||
		strings.Contains(event, `"type": "response.completed"`)
}

// isClientDisconnectError åˆ¤æ–­æ˜¯å¦ä¸ºå®¢æˆ·ç«¯æ–­å¼€è¿æ¥é”™è¯¯
func isClientDisconnectError(err error) bool {
	msg := err.Error()
	return strings.Contains(msg, "broken pipe") || strings.Contains(msg, "connection reset")
}

// injectResponsesUsageToCompletedEvent å‘ response.completed äº‹ä»¶æ³¨å…¥ usage
func injectResponsesUsageToCompletedEvent(event string, requestBody []byte, outputText string, envCfg *config.EnvConfig) string {
	inputTokens := utils.EstimateResponsesRequestTokens(requestBody)
	outputTokens := utils.EstimateTokens(outputText)
	totalTokens := inputTokens + outputTokens

	var result strings.Builder
	lines := strings.Split(event, "\n")

	for _, line := range lines {
		if !strings.HasPrefix(line, "data: ") {
			result.WriteString(line)
			result.WriteString("\n")
			continue
		}

		jsonStr := strings.TrimPrefix(line, "data: ")
		var data map[string]interface{}
		if err := json.Unmarshal([]byte(jsonStr), &data); err != nil {
			result.WriteString(line)
			result.WriteString("\n")
			continue
		}

		if data["type"] == "response.completed" {
			if response, ok := data["response"].(map[string]interface{}); ok {
				response["usage"] = map[string]interface{}{
					"input_tokens":  inputTokens,
					"output_tokens": outputTokens,
					"total_tokens":  totalTokens,
				}
			}

			patchedJSON, err := json.Marshal(data)
			if err != nil {
				result.WriteString(line)
				result.WriteString("\n")
				continue
			}

			if envCfg.EnableResponseLogs && envCfg.ShouldLog("debug") {
				log.Printf("ğŸ”¢ [Responses-Stream-Tokenæ³¨å…¥] InputTokens=%d, OutputTokens=%d, TotalTokens=%d",
					inputTokens, outputTokens, totalTokens)
			}

			result.WriteString("data: ")
			result.Write(patchedJSON)
			result.WriteString("\n")
		} else {
			result.WriteString(line)
			result.WriteString("\n")
		}
	}

	return result.String()
}

// patchResponsesCompletedEventUsage ä¿®è¡¥ response.completed äº‹ä»¶ä¸­çš„ usage
func patchResponsesCompletedEventUsage(event string, requestBody []byte, outputText string, collected *responsesStreamUsage, envCfg *config.EnvConfig) string {
	var result strings.Builder
	lines := strings.Split(event, "\n")

	for _, line := range lines {
		if !strings.HasPrefix(line, "data: ") {
			result.WriteString(line)
			result.WriteString("\n")
			continue
		}

		jsonStr := strings.TrimPrefix(line, "data: ")
		var data map[string]interface{}
		if err := json.Unmarshal([]byte(jsonStr), &data); err != nil {
			result.WriteString(line)
			result.WriteString("\n")
			continue
		}

		if data["type"] == "response.completed" {
			if response, ok := data["response"].(map[string]interface{}); ok {
				if usage, ok := response["usage"].(map[string]interface{}); ok {
					originalInput := collected.InputTokens
					originalOutput := collected.OutputTokens
					patched := false

					// ä¿®è¡¥ input_tokensï¼ˆä»…å½“æ²¡æœ‰ Claude åŸç”Ÿç¼“å­˜æ—¶ï¼‰
					// OpenAI çš„ cached_tokens ä¸åº”é˜»æ­¢ input_tokens è¡¥å…¨
					if collected.InputTokens <= 1 && !collected.HasClaudeCache {
						estimatedInput := utils.EstimateResponsesRequestTokens(requestBody)
						usage["input_tokens"] = estimatedInput
						collected.InputTokens = estimatedInput
						patched = true
					}

					// ä¿®è¡¥ output_tokens
					if collected.OutputTokens <= 1 {
						estimatedOutput := utils.EstimateTokens(outputText)
						usage["output_tokens"] = estimatedOutput
						collected.OutputTokens = estimatedOutput
						patched = true
					}

					// é‡æ–°è®¡ç®— total_tokensï¼ˆä¿®è¡¥æ—¶æˆ– total_tokens ä¸º 0 ä½† input/output æœ‰æ•ˆæ—¶ï¼‰
					currentTotal := 0
					if t, ok := usage["total_tokens"].(float64); ok {
						currentTotal = int(t)
					}
					if patched || (currentTotal == 0 && (collected.InputTokens > 0 || collected.OutputTokens > 0)) {
						usage["total_tokens"] = collected.InputTokens + collected.OutputTokens
					}

					if envCfg.EnableResponseLogs && envCfg.ShouldLog("debug") && patched {
						log.Printf("ğŸ”¢ [Responses-Stream-Tokenè¡¥å…¨] InputTokens=%dâ†’%d, OutputTokens=%dâ†’%d",
							originalInput, collected.InputTokens, originalOutput, collected.OutputTokens)
					}
				}
			}

			patchedJSON, err := json.Marshal(data)
			if err != nil {
				result.WriteString(line)
				result.WriteString("\n")
				continue
			}

			result.WriteString("data: ")
			result.Write(patchedJSON)
			result.WriteString("\n")
		} else {
			result.WriteString(line)
			result.WriteString("\n")
		}
	}

	return result.String()
}

// parseInputToItems è§£æ input ä¸º ResponsesItem æ•°ç»„
func parseInputToItems(input interface{}) ([]types.ResponsesItem, error) {
	switch v := input.(type) {
	case string:
		return []types.ResponsesItem{{Type: "text", Content: v}}, nil
	case []interface{}:
		items := []types.ResponsesItem{}
		for _, item := range v {
			itemMap, ok := item.(map[string]interface{})
			if !ok {
				continue
			}
			itemType, _ := itemMap["type"].(string)
			content := itemMap["content"]
			items = append(items, types.ResponsesItem{Type: itemType, Content: content})
		}
		return items, nil
	default:
		return nil, fmt.Errorf("unsupported input type")
	}
}
