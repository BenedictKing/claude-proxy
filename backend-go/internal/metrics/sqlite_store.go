package metrics

import (
	"database/sql"
	"fmt"
	"log"
	"os"
	"path/filepath"
	"sync"
	"time"

	_ "modernc.org/sqlite"
)

// SQLiteStore SQLite æŒä¹…åŒ–å­˜å‚¨
type SQLiteStore struct {
	db     *sql.DB
	dbPath string

	// å†™å…¥ç¼“å†²åŒº
	writeBuffer []PersistentRecord
	bufferMu    sync.Mutex

	// é…ç½®
	batchSize     int           // æ‰¹é‡å†™å…¥é˜ˆå€¼ï¼ˆè®°å½•æ•°ï¼‰
	flushInterval time.Duration // å®šæ—¶åˆ·æ–°é—´éš”
	retentionDays int           // æ•°æ®ä¿ç•™å¤©æ•°

	// æ§åˆ¶
	stopCh  chan struct{}
	wg      sync.WaitGroup
	closed  bool           // æ˜¯å¦å·²å…³é—­
	flushWg sync.WaitGroup // è¿½è¸ªå¼‚æ­¥ flush goroutine
}

// SQLiteStoreConfig SQLite å­˜å‚¨é…ç½®
type SQLiteStoreConfig struct {
	DBPath        string // æ•°æ®åº“æ–‡ä»¶è·¯å¾„
	RetentionDays int    // æ•°æ®ä¿ç•™å¤©æ•°ï¼ˆ3-30ï¼‰
}

// ç¡¬ç¼–ç çš„å†…éƒ¨é…ç½®
const (
	defaultBatchSize     = 100              // æ‰¹é‡å†™å…¥é˜ˆå€¼
	defaultFlushInterval = 30 * time.Second // å®šæ—¶åˆ·æ–°é—´éš”
)

// NewSQLiteStore åˆ›å»º SQLite å­˜å‚¨
func NewSQLiteStore(cfg *SQLiteStoreConfig) (*SQLiteStore, error) {
	if cfg == nil {
		cfg = &SQLiteStoreConfig{
			DBPath:        ".config/metrics.db",
			RetentionDays: 7,
		}
	}

	// éªŒè¯ä¿ç•™å¤©æ•°èŒƒå›´
	if cfg.RetentionDays < 3 {
		cfg.RetentionDays = 3
	} else if cfg.RetentionDays > 30 {
		cfg.RetentionDays = 30
	}

	// ç¡®ä¿ç›®å½•å­˜åœ¨
	dir := filepath.Dir(cfg.DBPath)
	if err := os.MkdirAll(dir, 0755); err != nil {
		return nil, fmt.Errorf("åˆ›å»ºæ•°æ®åº“ç›®å½•å¤±è´¥: %w", err)
	}

	// æ‰“å¼€æ•°æ®åº“è¿æ¥ï¼ˆWAL æ¨¡å¼ + NORMAL åŒæ­¥ï¼‰
	// modernc.org/sqlite ä½¿ç”¨ _pragma= è¯­æ³•è®¾ç½® PRAGMA
	dsn := cfg.DBPath + "?_pragma=journal_mode(WAL)&_pragma=synchronous(NORMAL)&_pragma=busy_timeout(5000)"
	db, err := sql.Open("sqlite", dsn)
	if err != nil {
		return nil, fmt.Errorf("æ‰“å¼€æ•°æ®åº“å¤±è´¥: %w", err)
	}

	// è®¾ç½®è¿æ¥æ± å‚æ•°
	db.SetMaxOpenConns(1) // SQLite å•å†™å…¥è¿æ¥
	db.SetMaxIdleConns(1)
	db.SetConnMaxLifetime(0) // ä¸é™åˆ¶è¿æ¥ç”Ÿå‘½å‘¨æœŸ

	// åˆå§‹åŒ–è¡¨ç»“æ„
	if err := initSchema(db); err != nil {
		db.Close()
		return nil, fmt.Errorf("åˆå§‹åŒ–æ•°æ®åº“ schema å¤±è´¥: %w", err)
	}

	store := &SQLiteStore{
		db:            db,
		dbPath:        cfg.DBPath,
		writeBuffer:   make([]PersistentRecord, 0, defaultBatchSize),
		batchSize:     defaultBatchSize,
		flushInterval: defaultFlushInterval,
		retentionDays: cfg.RetentionDays,
		stopCh:        make(chan struct{}),
	}

	// å¯åŠ¨åå°ä»»åŠ¡
	store.wg.Add(2)
	go store.flushLoop()
	go store.cleanupLoop()

	log.Printf("âœ… SQLite æŒ‡æ ‡å­˜å‚¨å·²åˆå§‹åŒ–: %s (ä¿ç•™ %d å¤©)", cfg.DBPath, cfg.RetentionDays)
	return store, nil
}

// initSchema åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„
func initSchema(db *sql.DB) error {
	schema := `
		-- è¯·æ±‚è®°å½•è¡¨
		CREATE TABLE IF NOT EXISTS request_records (
			id INTEGER PRIMARY KEY AUTOINCREMENT,
			metrics_key TEXT NOT NULL,
			base_url TEXT NOT NULL,
			key_mask TEXT NOT NULL,
			timestamp INTEGER NOT NULL,
			success INTEGER NOT NULL,
			input_tokens INTEGER DEFAULT 0,
			output_tokens INTEGER DEFAULT 0,
			cache_creation_tokens INTEGER DEFAULT 0,
			cache_read_tokens INTEGER DEFAULT 0,
			api_type TEXT NOT NULL DEFAULT 'messages'
		);

		-- ç´¢å¼•ï¼šæŒ‰ api_type å’Œæ—¶é—´æŸ¥è¯¢
		CREATE INDEX IF NOT EXISTS idx_records_api_type_timestamp
			ON request_records(api_type, timestamp);

		-- ç´¢å¼•ï¼šæŒ‰ metrics_key æŸ¥è¯¢
		CREATE INDEX IF NOT EXISTS idx_records_metrics_key
			ON request_records(metrics_key);
	`

	_, err := db.Exec(schema)
	return err
}

// AddRecord æ·»åŠ è®°å½•åˆ°å†™å…¥ç¼“å†²åŒºï¼ˆéé˜»å¡ï¼‰
func (s *SQLiteStore) AddRecord(record PersistentRecord) {
	s.bufferMu.Lock()
	if s.closed {
		s.bufferMu.Unlock()
		return // å·²å…³é—­ï¼Œå¿½ç•¥æ–°è®°å½•
	}
	s.writeBuffer = append(s.writeBuffer, record)
	shouldFlush := len(s.writeBuffer) >= s.batchSize
	s.bufferMu.Unlock()

	if shouldFlush {
		s.flushWg.Add(1)
		go func() {
			defer s.flushWg.Done()
			s.flush()
		}()
	}
}

// flush åˆ·æ–°ç¼“å†²åŒºåˆ°æ•°æ®åº“
func (s *SQLiteStore) flush() {
	s.bufferMu.Lock()
	if len(s.writeBuffer) == 0 {
		s.bufferMu.Unlock()
		return
	}

	// å–å‡ºç¼“å†²åŒºæ•°æ®
	records := s.writeBuffer
	s.writeBuffer = make([]PersistentRecord, 0, s.batchSize)
	s.bufferMu.Unlock()

	// æ‰¹é‡å†™å…¥
	if err := s.batchInsertRecords(records); err != nil {
		log.Printf("âš ï¸ æ‰¹é‡å†™å…¥æŒ‡æ ‡è®°å½•å¤±è´¥: %v", err)
		// å¤±è´¥æ—¶å°†è®°å½•æ”¾å›ç¼“å†²åŒºï¼ˆé™åˆ¶é‡è¯•ï¼Œé¿å…æ— é™å¢é•¿ï¼‰
		s.bufferMu.Lock()
		if len(s.writeBuffer) < s.batchSize*10 { // æœ€å¤šä¿ç•™ 10 å€ç¼“å†²
			s.writeBuffer = append(records, s.writeBuffer...)
		} else {
			log.Printf("âš ï¸ å†™å…¥ç¼“å†²åŒºå·²æ»¡ï¼Œä¸¢å¼ƒ %d æ¡è®°å½•", len(records))
		}
		s.bufferMu.Unlock()
	}
}

// batchInsertRecords æ‰¹é‡æ’å…¥è®°å½•
func (s *SQLiteStore) batchInsertRecords(records []PersistentRecord) error {
	if len(records) == 0 {
		return nil
	}

	tx, err := s.db.Begin()
	if err != nil {
		return err
	}
	defer tx.Rollback()

	stmt, err := tx.Prepare(`
		INSERT INTO request_records
		(metrics_key, base_url, key_mask, timestamp, success,
		 input_tokens, output_tokens, cache_creation_tokens, cache_read_tokens, api_type)
		VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
	`)
	if err != nil {
		return err
	}
	defer stmt.Close()

	for _, r := range records {
		success := 0
		if r.Success {
			success = 1
		}
		_, err := stmt.Exec(
			r.MetricsKey, r.BaseURL, r.KeyMask, r.Timestamp.Unix(), success,
			r.InputTokens, r.OutputTokens, r.CacheCreationTokens, r.CacheReadTokens, r.APIType,
		)
		if err != nil {
			return err
		}
	}

	return tx.Commit()
}

// LoadRecords åŠ è½½æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„è®°å½•
func (s *SQLiteStore) LoadRecords(since time.Time, apiType string) ([]PersistentRecord, error) {
	rows, err := s.db.Query(`
		SELECT metrics_key, base_url, key_mask, timestamp, success,
		       input_tokens, output_tokens, cache_creation_tokens, cache_read_tokens
		FROM request_records
		WHERE timestamp >= ? AND api_type = ?
		ORDER BY timestamp ASC
	`, since.Unix(), apiType)
	if err != nil {
		return nil, err
	}
	defer rows.Close()

	var records []PersistentRecord
	for rows.Next() {
		var r PersistentRecord
		var ts int64
		var success int

		err := rows.Scan(
			&r.MetricsKey, &r.BaseURL, &r.KeyMask, &ts, &success,
			&r.InputTokens, &r.OutputTokens, &r.CacheCreationTokens, &r.CacheReadTokens,
		)
		if err != nil {
			return nil, err
		}

		r.Timestamp = time.Unix(ts, 0)
		r.Success = success == 1
		r.APIType = apiType
		records = append(records, r)
	}

	return records, rows.Err()
}

// CleanupOldRecords æ¸…ç†è¿‡æœŸæ•°æ®
func (s *SQLiteStore) CleanupOldRecords(before time.Time) (int64, error) {
	result, err := s.db.Exec(
		"DELETE FROM request_records WHERE timestamp < ?",
		before.Unix(),
	)
	if err != nil {
		return 0, err
	}
	return result.RowsAffected()
}

// flushLoop å®šæ—¶åˆ·æ–°å¾ªç¯
func (s *SQLiteStore) flushLoop() {
	defer s.wg.Done()
	ticker := time.NewTicker(s.flushInterval)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			s.flush()
		case <-s.stopCh:
			s.flush() // å…³é—­å‰æœ€åä¸€æ¬¡åˆ·æ–°
			return
		}
	}
}

// cleanupLoop å®šæœŸæ¸…ç†å¾ªç¯
func (s *SQLiteStore) cleanupLoop() {
	defer s.wg.Done()

	// å¯åŠ¨æ—¶å…ˆæ¸…ç†ä¸€æ¬¡
	s.doCleanup()

	ticker := time.NewTicker(1 * time.Hour)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			s.doCleanup()
		case <-s.stopCh:
			return
		}
	}
}

// doCleanup æ‰§è¡Œæ¸…ç†
func (s *SQLiteStore) doCleanup() {
	cutoff := time.Now().AddDate(0, 0, -s.retentionDays)
	deleted, err := s.CleanupOldRecords(cutoff)
	if err != nil {
		log.Printf("âš ï¸ æ¸…ç†è¿‡æœŸæŒ‡æ ‡è®°å½•å¤±è´¥: %v", err)
	} else if deleted > 0 {
		log.Printf("ğŸ§¹ å·²æ¸…ç† %d æ¡è¿‡æœŸæŒ‡æ ‡è®°å½•ï¼ˆè¶…è¿‡ %d å¤©ï¼‰", deleted, s.retentionDays)
	}
}

// Close å…³é—­å­˜å‚¨
func (s *SQLiteStore) Close() error {
	// æ ‡è®°ä¸ºå·²å…³é—­ï¼Œé˜»æ­¢æ–°è®°å½•
	s.bufferMu.Lock()
	s.closed = true
	s.bufferMu.Unlock()

	// åœæ­¢åå°å¾ªç¯
	close(s.stopCh)
	s.wg.Wait()

	// ç­‰å¾…æ‰€æœ‰å¼‚æ­¥ flush å®Œæˆ
	s.flushWg.Wait()

	return s.db.Close()
}

// GetRecordCount è·å–è®°å½•æ€»æ•°ï¼ˆç”¨äºè°ƒè¯•ï¼‰
func (s *SQLiteStore) GetRecordCount() (int64, error) {
	var count int64
	err := s.db.QueryRow("SELECT COUNT(*) FROM request_records").Scan(&count)
	return count, err
}
